{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Shabarinath_Chandran_Week_4_Lab_Neural_Networks_in_Practice.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shabarinath8899/Shabarinath_Repository/blob/master/Shabarinath_Chandran_Week_4_Lab_Neural_Networks_in_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jE7Q3E9i9Evd"
      },
      "source": [
        "# Shabarinath_Chandran_Week 4 Lab: Neural Networks in practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": false,
        "id": "gWDjQyV59Evg",
        "colab": {},
        "outputId": "b7a6d500-9042-4fcf-c719-7af98ae15fe9"
      },
      "source": [
        "from IPython.display import set_matplotlib_formats, display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cycler import cycler\n",
        "import keras\n",
        "print(\"Using Keras\",keras.__version__)\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.dpi'] = 125 # Use 300 for PDF, 100 for slides\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using Keras 2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "center",
        "colab_type": "text",
        "id": "QJdIHypq9Evk"
      },
      "source": [
        "### Overview\n",
        "* Solving basic classification and regression problems\n",
        "* Handling textual data\n",
        "* Model selection (and overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "njT6MasM9Evl"
      },
      "source": [
        "## Solving basic problems\n",
        "* Binary classification (of movie reviews)\n",
        "* Multiclass classification (of news topics)\n",
        "* Regression (of house prices)\n",
        "\n",
        "Examples from _Deep Learning with Python_, by _Fran√ßois Chollet_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xsUfzIi59Evn"
      },
      "source": [
        "### Binary classification\n",
        "* Dataset: 50,000 IMDB reviews, labeled positive (1) or negative (0)\n",
        "    - Included in Keras, with a 50/50 train-test split\n",
        "* Each row is one review, with only the 10,000 most frequent words retained\n",
        "* Each word is replaced by a _word index_ (word ID)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "Mk42XDWZ9Evn",
        "colab": {},
        "outputId": "c4f7f4a6-ff00-4521-9b89-b6ed098c9051"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "# Download IMDB data with 10000 most frequent words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "print(\"Encoded review: \", train_data[0][0:10])\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "print(\"Original review: \", ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]][0:10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded review:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "Original review:  ? this film was just brilliant casting location scenery story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gbHSVzma9Evq"
      },
      "source": [
        "#### Preprocessing\n",
        "* We can't input lists of categorical value to a neural net, we need to create tensors\n",
        "* One-hot-encoding:\n",
        "    -  10000 features, '1.0' if the word occurs\n",
        "* Word embeddings (word2vec):\n",
        "    - Map each word to a dense vector that represents it (it's _embedding_)\n",
        "    - _Embedding_ layer: pre-trained layer that looks up the embedding in a dictionary \n",
        "    - Converts 2D tensor of word indices (zero-padded) to 3D tensor of embeddings\n",
        "* Let's do One-Hot-Encoding for now. We'll come back to _Embedding_ layers.\n",
        "* Also vectorize the labels: from 0/1 to float\n",
        "    - Binary classification works with one output node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "nSzfO-XG9Evs",
        "colab": {},
        "outputId": "5c76c59a-c668-4170-803a-d0190fbb9849"
      },
      "source": [
        "# Custom implementation of one-hot-encoding\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "arrays used as indices must be of integer (or boolean) type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b150dbd25148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m  \u001b[0;31m# set specific indices of results[i] to 1s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b150dbd25148>\u001b[0m in \u001b[0;36mvectorize_sequences\u001b[0;34m(sequences, dimension)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m  \u001b[0;31m# set specific indices of results[i] to 1s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4_JHofjQRme1"
      },
      "source": [
        "#### Understanding the format of IMDB dataset\n",
        "1. Train_data and test_data are an array of lists. What does the length of this array correspond to? What does the length of each list correspond to?\n",
        "2. What are the sizes of the vectorized x_train and x_test? What do the dimensions correspond to?\n",
        "3. What is the most common word in the first review in the training data? Hint: use the word index (see above)? \n",
        "4. Print the first review to verify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ceBrbGWxRLJK",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GVar7SNn9Evu"
      },
      "source": [
        "#### Building the network\n",
        "* We can solve this problem using a network of _Dense_ layers and the _ReLU_ activation function.\n",
        "* How many layers? How many hidden units for layer?\n",
        "    - Start with 2 layers of 16 hidden units each\n",
        "    - We'll optimize this soon\n",
        "* Output layer: single unit with _sigmoid_ activation function\n",
        "    - Close to 1: positive review, close to 0: negative review\n",
        "* Use binary_crossentropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wglvzu9E9Evx",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0S_Fnh169Ev0"
      },
      "source": [
        "#### Model selection\n",
        "* How many epochs do we need for training?\n",
        "* Take a validation set of 10,000 samples from the training set\n",
        "* Train the neural net and track the loss after every iteration on the validation set\n",
        "    - This is returned as a `History` object by the `fit()` function \n",
        "* We start with 20 epochs in minibatches of 512 samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b3aTf8rL9Ev0",
        "colab": {},
        "outputId": "eedc62b6-1088-48cb-94fd-f74b1713ca81"
      },
      "source": [
        "x_val, partial_x_train = x_train[:10000], x_train[10000:]\n",
        "y_val, partial_y_train = y_train[:10000], y_train[10000:] \n",
        "history = model.fit(partial_x_train, partial_y_train,\n",
        "                    epochs=20, batch_size=512, verbose=2,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 4s - loss: 0.5063 - accuracy: 0.7904 - val_loss: 0.3947 - val_accuracy: 0.8481\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.3019 - accuracy: 0.9013 - val_loss: 0.3047 - val_accuracy: 0.8854\n",
            "Epoch 3/20\n",
            " - 1s - loss: 0.2216 - accuracy: 0.9277 - val_loss: 0.3035 - val_accuracy: 0.8746\n",
            "Epoch 4/20\n",
            " - 1s - loss: 0.1790 - accuracy: 0.9425 - val_loss: 0.2858 - val_accuracy: 0.8826\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.1460 - accuracy: 0.9531 - val_loss: 0.2853 - val_accuracy: 0.8853\n",
            "Epoch 6/20\n",
            " - 1s - loss: 0.1184 - accuracy: 0.9642 - val_loss: 0.3024 - val_accuracy: 0.8831\n",
            "Epoch 7/20\n",
            " - 1s - loss: 0.1019 - accuracy: 0.9682 - val_loss: 0.3100 - val_accuracy: 0.8822\n",
            "Epoch 8/20\n",
            " - 1s - loss: 0.0804 - accuracy: 0.9783 - val_loss: 0.3435 - val_accuracy: 0.8789\n",
            "Epoch 9/20\n",
            " - 1s - loss: 0.0686 - accuracy: 0.9796 - val_loss: 0.3715 - val_accuracy: 0.8759\n",
            "Epoch 10/20\n",
            " - 1s - loss: 0.0553 - accuracy: 0.9857 - val_loss: 0.3742 - val_accuracy: 0.8754\n",
            "Epoch 11/20\n",
            " - 1s - loss: 0.0449 - accuracy: 0.9889 - val_loss: 0.3978 - val_accuracy: 0.8750\n",
            "Epoch 12/20\n",
            " - 1s - loss: 0.0342 - accuracy: 0.9927 - val_loss: 0.4412 - val_accuracy: 0.8704\n",
            "Epoch 13/20\n",
            " - 1s - loss: 0.0279 - accuracy: 0.9950 - val_loss: 0.4614 - val_accuracy: 0.8743\n",
            "Epoch 14/20\n",
            " - 1s - loss: 0.0214 - accuracy: 0.9961 - val_loss: 0.4985 - val_accuracy: 0.8733\n",
            "Epoch 15/20\n",
            " - 1s - loss: 0.0153 - accuracy: 0.9985 - val_loss: 0.5382 - val_accuracy: 0.8725\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.0125 - accuracy: 0.9989 - val_loss: 0.5719 - val_accuracy: 0.8703\n",
            "Epoch 17/20\n",
            " - 1s - loss: 0.0101 - accuracy: 0.9986 - val_loss: 0.6061 - val_accuracy: 0.8688\n",
            "Epoch 18/20\n",
            " - 1s - loss: 0.0066 - accuracy: 0.9992 - val_loss: 0.6459 - val_accuracy: 0.8654\n",
            "Epoch 19/20\n",
            " - 1s - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.6795 - val_accuracy: 0.8675\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.0049 - accuracy: 0.9993 - val_loss: 0.7129 - val_accuracy: 0.8661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PrxYLkMX9Ev3"
      },
      "source": [
        "#### Evaluate model performance during training\n",
        "1. Plot the training and validation loss as a function of training epoch. Describe what happens during the training in terms of under or overfitting.\n",
        "2. Plot the training and validation accuracy as a function of the training epoch.\n",
        "\n",
        "Hint: these quantities are contained in the dict history.history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "4YH3yVZy9Ev4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hide_input": true,
        "id": "wrJhYEe09Ev-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWDq34yW9EwA"
      },
      "source": [
        "#### Early stopping\n",
        "One simple technique to avoid overfitting is to use the validation set to 'tune' the optimal number of epochs\n",
        "* In this case, we could stop after 4 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "Csk3_Yn09EwB",
        "colab": {},
        "outputId": "22780d07-fc66-4921-e0bd-249096f7c537"
      },
      "source": [
        "#@title\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512, verbose=2)\n",
        "result = model.evaluate(x_test, y_test)\n",
        "print(\"Loss: {:.4f}, Accuracy:  {:.4f}\".format(*result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            " - 2s - loss: 0.2283 - accuracy: 0.9462\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.1320 - accuracy: 0.9608\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.0996 - accuracy: 0.9696\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.0765 - accuracy: 0.9763\n",
            "25000/25000 [==============================] - 5s 192us/step\n",
            "Loss: 0.4862, Accuracy:  0.8598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Mo4BX1N9EwD"
      },
      "source": [
        "#### Predictions\n",
        "1. Print the first review that were correctly classified along with the predicted value.\n",
        "2. Print the first review that were misclassified along with the predicted value. Can you explain why the model likely failed? How confident was the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fZB5xKAh9EwE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W9_hvA-lanwt"
      },
      "source": [
        "#### Takeaways\n",
        "* Neural nets require a lot of preprocessing to create tensors\n",
        "* Dense layers with ReLU activation can solve a wide range of problems\n",
        "* Binary classification can be done with a Dense layer with a single unit, sigmoid activation, and binary cross-entropy loss\n",
        "* Neural nets overfit easily\n",
        "* Many design choices have an effect on accuracy and overfitting. One can try:\n",
        "    - 1 or 3 hidden layers\n",
        "    - more or fewer hidden units (e.g. 64)\n",
        "    - MSE loss instead of binary cross-entropy\n",
        "    - `tanh` activation instead of `ReLU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WchP4Tgu9ExF"
      },
      "source": [
        "### Regularization: build smaller networks\n",
        "* The easiest way to avoid overfitting is to use a simpler model\n",
        "* The number of learnable parameters is called the model _capacity_\n",
        "* A model with more parameters has a higher _memorization capacity_\n",
        "    - The entire training set can be `stored` in the weights\n",
        "    - Learns the mapping from training examples to outputs\n",
        "* Forcing the model to be small forces it to learn a compressed representation that generalizes better\n",
        "    - Always a trade-off between too much and too little capacity\n",
        "* Start with few layers and parameters, incease until you see diminisching returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wo8UGPGQ9ExG"
      },
      "source": [
        "Let's try this on our movie review data, with 4 units per layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFjmZtyl9ExG",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RYsX-RF9ExJ",
        "colab": {},
        "outputId": "e565c2be-c546-4542-a7c4-6be21ef95d58"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_model = models.Sequential()\n",
        "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "original_model.add(layers.Dense(16, activation='relu'))\n",
        "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "original_model.compile(optimizer='rmsprop',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "\n",
        "smaller_model = models.Sequential()\n",
        "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
        "smaller_model.add(layers.Dense(4, activation='relu'))\n",
        "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "smaller_model.compile(optimizer='rmsprop',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "original_hist = original_model.fit(x_train, y_train,\n",
        "                                   epochs=20,\n",
        "                                   batch_size=512, verbose=2,\n",
        "                                   validation_data=(x_test, y_test))\n",
        "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
        "                                       epochs=20,\n",
        "                                       batch_size=512, verbose=2,\n",
        "                                       validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            " - 7s - loss: 0.4820 - acc: 0.8104 - val_loss: 0.3584 - val_acc: 0.8797\n",
            "Epoch 2/20\n",
            " - 4s - loss: 0.2737 - acc: 0.9060 - val_loss: 0.2884 - val_acc: 0.8897\n",
            "Epoch 3/20\n",
            " - 3s - loss: 0.2076 - acc: 0.9249 - val_loss: 0.2802 - val_acc: 0.8886\n",
            "Epoch 4/20\n",
            " - 3s - loss: 0.1708 - acc: 0.9390 - val_loss: 0.2904 - val_acc: 0.8849\n",
            "Epoch 5/20\n",
            " - 3s - loss: 0.1487 - acc: 0.9490 - val_loss: 0.3131 - val_acc: 0.8797\n",
            "Epoch 6/20\n",
            " - 3s - loss: 0.1311 - acc: 0.9547 - val_loss: 0.3312 - val_acc: 0.8757\n",
            "Epoch 7/20\n",
            " - 3s - loss: 0.1128 - acc: 0.9617 - val_loss: 0.3596 - val_acc: 0.8727\n",
            "Epoch 8/20\n",
            " - 3s - loss: 0.1013 - acc: 0.9657 - val_loss: 0.3783 - val_acc: 0.8701\n",
            "Epoch 9/20\n",
            " - 3s - loss: 0.0888 - acc: 0.9702 - val_loss: 0.4062 - val_acc: 0.8658\n",
            "Epoch 10/20\n",
            " - 3s - loss: 0.0780 - acc: 0.9751 - val_loss: 0.4325 - val_acc: 0.8664\n",
            "Epoch 11/20\n",
            " - 4s - loss: 0.0682 - acc: 0.9781 - val_loss: 0.4614 - val_acc: 0.8629\n",
            "Epoch 12/20\n",
            " - 3s - loss: 0.0595 - acc: 0.9814 - val_loss: 0.5156 - val_acc: 0.8619\n",
            "Epoch 13/20\n",
            " - 3s - loss: 0.0503 - acc: 0.9851 - val_loss: 0.5499 - val_acc: 0.8521\n",
            "Epoch 14/20\n",
            " - 3s - loss: 0.0435 - acc: 0.9874 - val_loss: 0.6235 - val_acc: 0.8433\n",
            "Epoch 15/20\n",
            " - 4s - loss: 0.0379 - acc: 0.9894 - val_loss: 0.6056 - val_acc: 0.8572\n",
            "Epoch 16/20\n",
            " - 3s - loss: 0.0326 - acc: 0.9914 - val_loss: 0.6303 - val_acc: 0.8542\n",
            "Epoch 17/20\n",
            " - 3s - loss: 0.0262 - acc: 0.9932 - val_loss: 0.6738 - val_acc: 0.8545\n",
            "Epoch 18/20\n",
            " - 3s - loss: 0.0233 - acc: 0.9941 - val_loss: 0.7161 - val_acc: 0.8532\n",
            "Epoch 19/20\n",
            " - 3s - loss: 0.0183 - acc: 0.9954 - val_loss: 0.7716 - val_acc: 0.8526\n",
            "Epoch 20/20\n",
            " - 3s - loss: 0.0171 - acc: 0.9962 - val_loss: 0.7846 - val_acc: 0.8508\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            " - 3s - loss: 0.5244 - acc: 0.8084 - val_loss: 0.4222 - val_acc: 0.8706\n",
            "Epoch 2/20\n",
            " - 3s - loss: 0.3376 - acc: 0.9012 - val_loss: 0.3294 - val_acc: 0.8862\n",
            "Epoch 3/20\n",
            " - 4s - loss: 0.2577 - acc: 0.9178 - val_loss: 0.2979 - val_acc: 0.8859\n",
            "Epoch 4/20\n",
            " - 3s - loss: 0.2133 - acc: 0.9291 - val_loss: 0.2897 - val_acc: 0.8833\n",
            "Epoch 5/20\n",
            " - 3s - loss: 0.1842 - acc: 0.9370 - val_loss: 0.2796 - val_acc: 0.8876\n",
            "Epoch 6/20\n",
            " - 3s - loss: 0.1637 - acc: 0.9455 - val_loss: 0.2840 - val_acc: 0.8860\n",
            "Epoch 7/20\n",
            " - 3s - loss: 0.1470 - acc: 0.9507 - val_loss: 0.2943 - val_acc: 0.8827\n",
            "Epoch 8/20\n",
            " - 3s - loss: 0.1348 - acc: 0.9552 - val_loss: 0.3046 - val_acc: 0.8812\n",
            "Epoch 9/20\n",
            " - 3s - loss: 0.1233 - acc: 0.9601 - val_loss: 0.3283 - val_acc: 0.8757\n",
            "Epoch 10/20\n",
            " - 3s - loss: 0.1137 - acc: 0.9641 - val_loss: 0.3320 - val_acc: 0.8770\n",
            "Epoch 11/20\n",
            " - 3s - loss: 0.1045 - acc: 0.9674 - val_loss: 0.3464 - val_acc: 0.8753\n",
            "Epoch 12/20\n",
            " - 3s - loss: 0.0974 - acc: 0.9698 - val_loss: 0.3674 - val_acc: 0.8707\n",
            "Epoch 13/20\n",
            " - 3s - loss: 0.0902 - acc: 0.9733 - val_loss: 0.3817 - val_acc: 0.8693\n",
            "Epoch 14/20\n",
            " - 3s - loss: 0.0833 - acc: 0.9754 - val_loss: 0.3980 - val_acc: 0.8679\n",
            "Epoch 15/20\n",
            " - 3s - loss: 0.0779 - acc: 0.9774 - val_loss: 0.4161 - val_acc: 0.8653\n",
            "Epoch 16/20\n",
            " - 3s - loss: 0.0716 - acc: 0.9793 - val_loss: 0.4378 - val_acc: 0.8635\n",
            "Epoch 17/20\n",
            " - 3s - loss: 0.0667 - acc: 0.9819 - val_loss: 0.4624 - val_acc: 0.8607\n",
            "Epoch 18/20\n",
            " - 3s - loss: 0.0621 - acc: 0.9827 - val_loss: 0.4804 - val_acc: 0.8598\n",
            "Epoch 19/20\n",
            " - 6s - loss: 0.0577 - acc: 0.9840 - val_loss: 0.5019 - val_acc: 0.8581\n",
            "Epoch 20/20\n",
            " - 5s - loss: 0.0529 - acc: 0.9860 - val_loss: 0.5215 - val_acc: 0.8573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rCV6TJ899ExL"
      },
      "source": [
        "1. Plot the validation loss for the original and smaller models. How does the smaller model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m7kNvhh-9ExN",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_u-YXpCZ9ExP"
      },
      "source": [
        "### Regularization: Weight regularization\n",
        "* As we did many times before, we can also add weight regularization to our loss function\n",
        "- L1 regularization: leads to _sparse networks_ with many weights that are 0\n",
        "- L2 regularization: leads to many very small weights\n",
        "    - Also called _weight decay_ in neural net literature\n",
        "* In Keras, add `kernel_regularizer` to every layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXOqh2uU9ExR",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "l2_model = models.Sequential()\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu', input_shape=(10000,)))\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "l2_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mzNX3bHo9ExT",
        "colab": {}
      },
      "source": [
        "l2_model.compile(optimizer='rmsprop',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Gg8hLY69ExU",
        "colab": {},
        "outputId": "3734f19d-cb22-4169-ce1c-36d32a105ffc"
      },
      "source": [
        "l2_model_hist = l2_model.fit(x_train, y_train,\n",
        "                             epochs=20,\n",
        "                             batch_size=512, verbose=2,\n",
        "                             validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            " - 3s - loss: 0.5090 - acc: 0.8158 - val_loss: 0.3796 - val_acc: 0.8831\n",
            "Epoch 2/20\n",
            " - 3s - loss: 0.3184 - acc: 0.9048 - val_loss: 0.3366 - val_acc: 0.8888\n",
            "Epoch 3/20\n",
            " - 3s - loss: 0.2734 - acc: 0.9213 - val_loss: 0.3355 - val_acc: 0.8881\n",
            "Epoch 4/20\n",
            " - 3s - loss: 0.2572 - acc: 0.9247 - val_loss: 0.3355 - val_acc: 0.8882\n",
            "Epoch 5/20\n",
            " - 3s - loss: 0.2412 - acc: 0.9329 - val_loss: 0.3432 - val_acc: 0.8843\n",
            "Epoch 6/20\n",
            " - 3s - loss: 0.2349 - acc: 0.9366 - val_loss: 0.3488 - val_acc: 0.8824\n",
            "Epoch 7/20\n",
            " - 3s - loss: 0.2317 - acc: 0.9359 - val_loss: 0.3644 - val_acc: 0.8761\n",
            "Epoch 8/20\n",
            " - 3s - loss: 0.2228 - acc: 0.9410 - val_loss: 0.3634 - val_acc: 0.8789\n",
            "Epoch 9/20\n",
            " - 3s - loss: 0.2213 - acc: 0.9395 - val_loss: 0.3686 - val_acc: 0.8778\n",
            "Epoch 10/20\n",
            " - 4s - loss: 0.2164 - acc: 0.9414 - val_loss: 0.3860 - val_acc: 0.8743\n",
            "Epoch 11/20\n",
            " - 3s - loss: 0.2174 - acc: 0.9413 - val_loss: 0.3916 - val_acc: 0.8708\n",
            "Epoch 12/20\n",
            " - 3s - loss: 0.2085 - acc: 0.9455 - val_loss: 0.3841 - val_acc: 0.8741\n",
            "Epoch 13/20\n",
            " - 3s - loss: 0.2089 - acc: 0.9450 - val_loss: 0.3995 - val_acc: 0.8713\n",
            "Epoch 14/20\n",
            " - 3s - loss: 0.2091 - acc: 0.9440 - val_loss: 0.3939 - val_acc: 0.8723\n",
            "Epoch 15/20\n",
            " - 3s - loss: 0.2025 - acc: 0.9465 - val_loss: 0.4021 - val_acc: 0.8706\n",
            "Epoch 16/20\n",
            " - 3s - loss: 0.2020 - acc: 0.9472 - val_loss: 0.4112 - val_acc: 0.8688\n",
            "Epoch 17/20\n",
            " - 3s - loss: 0.2011 - acc: 0.9468 - val_loss: 0.4065 - val_acc: 0.8709\n",
            "Epoch 18/20\n",
            " - 3s - loss: 0.1987 - acc: 0.9473 - val_loss: 0.4151 - val_acc: 0.8694\n",
            "Epoch 19/20\n",
            " - 3s - loss: 0.2025 - acc: 0.9461 - val_loss: 0.4635 - val_acc: 0.8525\n",
            "Epoch 20/20\n",
            " - 3s - loss: 0.1920 - acc: 0.9506 - val_loss: 0.4499 - val_acc: 0.8578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKOw1jNlibXK"
      },
      "source": [
        "1. Plot the validation loss for the original and l2 regularized models. How does the regularized model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9qYQvkIe9ExW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFbL6yJF9ExZ"
      },
      "source": [
        "### Regularization: dropout\n",
        "* One of the most effective and commonly used regularization techniques\n",
        "* Randomly set a number of outputs of the layer to 0\n",
        "* Idea: break up accidental non-significant learned patterns \n",
        "* _Dropout rate_: fraction of the outputs that are zeroed-out\n",
        "    - Usually between 0.2 and 0.5\n",
        "* At test time, nothing is dropped out, but the output values are scaled down by the dropout rate\n",
        "    - Balances out that more units are active than during training\n",
        "* In Keras: add `Dropout` layers between the normal layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SUHgbTz49Exa",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dpt_model = models.Sequential()\n",
        "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(16, activation='relu'))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "dpt_model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QSG_vWa9Exb",
        "colab": {},
        "outputId": "c29f0f5b-7cf8-4ea0-8991-fa3cbf7431a0"
      },
      "source": [
        "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
        "                               epochs=20,\n",
        "                               \n",
        "                               batch_size=512, verbose=2,\n",
        "                               validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            " - 4s - loss: 0.5770 - acc: 0.6942 - val_loss: 0.4178 - val_acc: 0.8679\n",
            "Epoch 2/20\n",
            " - 3s - loss: 0.4158 - acc: 0.8284 - val_loss: 0.3183 - val_acc: 0.8839\n",
            "Epoch 3/20\n",
            " - 3s - loss: 0.3375 - acc: 0.8702 - val_loss: 0.2850 - val_acc: 0.8885\n",
            "Epoch 4/20\n",
            " - 3s - loss: 0.2893 - acc: 0.8941 - val_loss: 0.2743 - val_acc: 0.8910\n",
            "Epoch 5/20\n",
            " - 3s - loss: 0.2511 - acc: 0.9092 - val_loss: 0.2781 - val_acc: 0.8894\n",
            "Epoch 6/20\n",
            " - 3s - loss: 0.2257 - acc: 0.9194 - val_loss: 0.2885 - val_acc: 0.8883\n",
            "Epoch 7/20\n",
            " - 3s - loss: 0.2045 - acc: 0.9283 - val_loss: 0.3289 - val_acc: 0.8801\n",
            "Epoch 8/20\n",
            " - 3s - loss: 0.1872 - acc: 0.9328 - val_loss: 0.3280 - val_acc: 0.8854\n",
            "Epoch 9/20\n",
            " - 3s - loss: 0.1741 - acc: 0.9372 - val_loss: 0.3430 - val_acc: 0.8834\n",
            "Epoch 10/20\n",
            " - 3s - loss: 0.1626 - acc: 0.9429 - val_loss: 0.3601 - val_acc: 0.8829\n",
            "Epoch 11/20\n",
            " - 3s - loss: 0.1521 - acc: 0.9458 - val_loss: 0.3986 - val_acc: 0.8802\n",
            "Epoch 12/20\n",
            " - 3s - loss: 0.1452 - acc: 0.9479 - val_loss: 0.4212 - val_acc: 0.8810\n",
            "Epoch 13/20\n",
            " - 3s - loss: 0.1392 - acc: 0.9485 - val_loss: 0.4325 - val_acc: 0.8806\n",
            "Epoch 14/20\n",
            " - 3s - loss: 0.1303 - acc: 0.9527 - val_loss: 0.4524 - val_acc: 0.8785\n",
            "Epoch 15/20\n",
            " - 3s - loss: 0.1256 - acc: 0.9537 - val_loss: 0.5009 - val_acc: 0.8795\n",
            "Epoch 16/20\n",
            " - 3s - loss: 0.1217 - acc: 0.9562 - val_loss: 0.5036 - val_acc: 0.8776\n",
            "Epoch 17/20\n",
            " - 3s - loss: 0.1213 - acc: 0.9544 - val_loss: 0.5263 - val_acc: 0.8771\n",
            "Epoch 18/20\n",
            " - 3s - loss: 0.1153 - acc: 0.9558 - val_loss: 0.5482 - val_acc: 0.8756\n",
            "Epoch 19/20\n",
            " - 3s - loss: 0.1128 - acc: 0.9572 - val_loss: 0.5570 - val_acc: 0.8748\n",
            "Epoch 20/20\n",
            " - 3s - loss: 0.1128 - acc: 0.9567 - val_loss: 0.5808 - val_acc: 0.8738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VZx6VNXn9Exd"
      },
      "source": [
        "1. Plot the validation loss for the original and dropout models. How does the dropout model behave compared to the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZS_z7xM9Exe",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ORAwOqdV9Exf"
      },
      "source": [
        "### Regularization recap\n",
        "* Get more training data\n",
        "* Reduce the capacity of the network\n",
        "* Add weight regularization\n",
        "* Add dropout\n",
        "* Either start with a simple model and add capacity\n",
        "* Or, start with a complex model and then regularize by adding weight regularization and dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xwfZv_E9Ewq"
      },
      "source": [
        "### Regression\n",
        "* Dataset: 506 examples of houses and sale prices (Boston)\n",
        "    - Included in Keras, with a 1/5 train-test split\n",
        "* Each row is one house price, described by numeric properties of the house and neighborhood\n",
        "* Small dataset, non-normalized features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlUGyLJS9Ewr",
        "colab": {},
        "outputId": "c12718b3-bcb7-4750-ab4c-5c8b97a73a70"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gX4hkxBb9Ewt"
      },
      "source": [
        "#### Preprocessing\n",
        "* Neural nets work a lot better if we normalize the features first. \n",
        "* Keras has no built-in support so we have to do this manually (or with scikit-learn)\n",
        "    - Again, be careful not to look at the test data during normalization\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-W92M6I9Ewt",
        "colab": {}
      },
      "source": [
        "mean, std = train_data.mean(axis=0), train_data.std(axis=0)\n",
        "train_data -= mean\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwM0wfKE9Ewx"
      },
      "source": [
        "\n",
        "\n",
        "#### Building the network\n",
        "* This is a small dataset, so easy to overfit\n",
        "    * We use 2 hidden layers of 64 units each\n",
        "* Use smaller batches, more epochs\n",
        "* Since we want scalar output, the output layer is one unit without activation\n",
        "* Loss function is Mean Squared Error (bigger penalty)\n",
        "* Evaluation metric is Mean Absolute Error (more interpretable)\n",
        "* We will also use cross-validation, so we wrap the model building in a function, so that we can call it multiple times\n",
        "\n",
        "1. Create a function build_model that returns the neural network model described above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vL_p-NcX9Ewz",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu',\n",
        "                           input_shape=(train_data.shape[1],)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2seh8XW9Ew1"
      },
      "source": [
        "#### Cross-validation\n",
        "* Keras does not have support for cross-validation\n",
        "* We can implement cross-validation ourselves (seeprovided code below)\n",
        "* Alternatively, we can wrap a Keras model as a scikit-learn estimator\n",
        "* Generally speaking, cross-validation is tricky with neural nets\n",
        "    * Some fold may not converge, or fluctuate on random initialization\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VupbDC1U9Ew3",
        "colab": {},
        "outputId": "416cd832-bb2b-42a4-ac93-1af86a5cc9cc"
      },
      "source": [
        "# implementation of cross-validation\n",
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 20\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    # Evaluate the model on the validation data\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=2)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcEyK-E59Ew8"
      },
      "source": [
        "1. Train for longer (200 epochs) and keep track of loss after every epoch. Plot and describe the loss as a function of epoch number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXsVlXqw9Ew8",
        "colab": {},
        "outputId": "014f9daf-b389-46c1-f5c8-0d4e66bc1689"
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session() # Memory clean-up\n",
        "\n",
        "num_epochs = 200\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=1, verbose=2)\n",
        "    mae_history = history.history['val_loss']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Train on 303 samples, validate on 101 samples\n",
            "Epoch 1/200\n",
            " - 1s - loss: 195.1275 - mae: 10.4902 - val_loss: 40.3629 - val_mae: 4.2612\n",
            "Epoch 2/200\n",
            " - 0s - loss: 36.0341 - mae: 4.1567 - val_loss: 29.7992 - val_mae: 3.4703\n",
            "Epoch 3/200\n",
            " - 0s - loss: 25.0092 - mae: 3.4746 - val_loss: 22.0133 - val_mae: 2.8533\n",
            "Epoch 4/200\n",
            " - 0s - loss: 21.2176 - mae: 3.0766 - val_loss: 17.2670 - val_mae: 2.7052\n",
            "Epoch 5/200\n",
            " - 1s - loss: 17.7324 - mae: 2.8494 - val_loss: 15.6532 - val_mae: 2.4966\n",
            "Epoch 6/200\n",
            " - 1s - loss: 15.8410 - mae: 2.6892 - val_loss: 13.8507 - val_mae: 2.4598\n",
            "Epoch 7/200\n",
            " - 1s - loss: 14.8836 - mae: 2.5972 - val_loss: 11.8518 - val_mae: 2.2289\n",
            "Epoch 8/200\n",
            " - 0s - loss: 13.6608 - mae: 2.4127 - val_loss: 13.1764 - val_mae: 2.3872\n",
            "Epoch 9/200\n",
            " - 0s - loss: 13.5292 - mae: 2.4517 - val_loss: 10.8733 - val_mae: 2.0731\n",
            "Epoch 10/200\n",
            " - 0s - loss: 12.6883 - mae: 2.3830 - val_loss: 11.3246 - val_mae: 2.2701\n",
            "Epoch 11/200\n",
            " - 0s - loss: 12.4358 - mae: 2.3276 - val_loss: 11.0299 - val_mae: 2.2772\n",
            "Epoch 12/200\n",
            " - 0s - loss: 11.4227 - mae: 2.3025 - val_loss: 15.0216 - val_mae: 2.7056\n",
            "Epoch 13/200\n",
            " - 0s - loss: 11.2469 - mae: 2.2765 - val_loss: 9.9369 - val_mae: 2.1486\n",
            "Epoch 14/200\n",
            " - 0s - loss: 11.6499 - mae: 2.2858 - val_loss: 8.8454 - val_mae: 2.1369\n",
            "Epoch 15/200\n",
            " - 0s - loss: 10.8885 - mae: 2.2205 - val_loss: 8.6988 - val_mae: 2.1318\n",
            "Epoch 16/200\n",
            " - 0s - loss: 10.4616 - mae: 2.1824 - val_loss: 10.2974 - val_mae: 2.4692\n",
            "Epoch 17/200\n",
            " - 0s - loss: 10.1238 - mae: 2.1331 - val_loss: 8.5848 - val_mae: 2.0073\n",
            "Epoch 18/200\n",
            " - 0s - loss: 10.2268 - mae: 2.1785 - val_loss: 8.3157 - val_mae: 1.9783\n",
            "Epoch 19/200\n",
            " - 0s - loss: 9.5413 - mae: 2.0604 - val_loss: 10.5569 - val_mae: 2.5183\n",
            "Epoch 20/200\n",
            " - 0s - loss: 9.6053 - mae: 2.1250 - val_loss: 8.4398 - val_mae: 2.1820\n",
            "Epoch 21/200\n",
            " - 0s - loss: 9.4005 - mae: 2.0585 - val_loss: 9.1054 - val_mae: 2.2980\n",
            "Epoch 22/200\n",
            " - 0s - loss: 8.9641 - mae: 2.0312 - val_loss: 7.9621 - val_mae: 1.8733\n",
            "Epoch 23/200\n",
            " - 0s - loss: 9.1899 - mae: 2.0259 - val_loss: 8.4252 - val_mae: 2.1380\n",
            "Epoch 24/200\n",
            " - 1s - loss: 8.7744 - mae: 2.0368 - val_loss: 8.7753 - val_mae: 2.2431\n",
            "Epoch 25/200\n",
            " - 0s - loss: 9.0762 - mae: 2.0479 - val_loss: 8.4276 - val_mae: 2.1991\n",
            "Epoch 26/200\n",
            " - 0s - loss: 8.2571 - mae: 2.0088 - val_loss: 7.4627 - val_mae: 1.9675\n",
            "Epoch 27/200\n",
            " - 0s - loss: 8.4724 - mae: 1.9416 - val_loss: 7.5965 - val_mae: 2.0803\n",
            "Epoch 28/200\n",
            " - 0s - loss: 8.2218 - mae: 1.9556 - val_loss: 8.7291 - val_mae: 2.2095\n",
            "Epoch 29/200\n",
            " - 0s - loss: 7.8139 - mae: 1.9015 - val_loss: 10.5799 - val_mae: 2.5268\n",
            "Epoch 30/200\n",
            " - 1s - loss: 7.8299 - mae: 1.9094 - val_loss: 7.7390 - val_mae: 2.1116\n",
            "Epoch 31/200\n",
            " - 1s - loss: 7.8061 - mae: 1.8986 - val_loss: 7.6332 - val_mae: 2.0172\n",
            "Epoch 32/200\n",
            " - 1s - loss: 7.9757 - mae: 1.8178 - val_loss: 8.1769 - val_mae: 2.2040\n",
            "Epoch 33/200\n",
            " - 1s - loss: 8.0176 - mae: 1.8534 - val_loss: 7.6679 - val_mae: 2.0773\n",
            "Epoch 34/200\n",
            " - 0s - loss: 7.3212 - mae: 1.8003 - val_loss: 7.8230 - val_mae: 2.0348\n",
            "Epoch 35/200\n",
            " - 0s - loss: 7.3993 - mae: 1.8028 - val_loss: 8.4499 - val_mae: 1.8703\n",
            "Epoch 36/200\n",
            " - 1s - loss: 7.5004 - mae: 1.8168 - val_loss: 7.6467 - val_mae: 2.0556\n",
            "Epoch 37/200\n",
            " - 1s - loss: 6.8215 - mae: 1.7302 - val_loss: 8.3354 - val_mae: 2.1785\n",
            "Epoch 38/200\n",
            " - 0s - loss: 7.1950 - mae: 1.7810 - val_loss: 8.2457 - val_mae: 1.8404\n",
            "Epoch 39/200\n",
            " - 1s - loss: 7.1400 - mae: 1.7612 - val_loss: 8.2021 - val_mae: 1.9899\n",
            "Epoch 40/200\n",
            " - 1s - loss: 6.5731 - mae: 1.7687 - val_loss: 9.4932 - val_mae: 2.1993\n",
            "Epoch 41/200\n",
            " - 1s - loss: 6.9030 - mae: 1.7943 - val_loss: 7.9476 - val_mae: 2.2026\n",
            "Epoch 42/200\n",
            " - 1s - loss: 6.6375 - mae: 1.7381 - val_loss: 8.0045 - val_mae: 2.0876\n",
            "Epoch 43/200\n",
            " - 0s - loss: 6.5692 - mae: 1.7085 - val_loss: 8.0067 - val_mae: 2.1781\n",
            "Epoch 44/200\n",
            " - 0s - loss: 6.4810 - mae: 1.7013 - val_loss: 7.5793 - val_mae: 1.9065\n",
            "Epoch 45/200\n",
            " - 0s - loss: 6.5586 - mae: 1.6407 - val_loss: 7.4957 - val_mae: 1.8909\n",
            "Epoch 46/200\n",
            " - 0s - loss: 6.4330 - mae: 1.6796 - val_loss: 7.1950 - val_mae: 2.0882\n",
            "Epoch 47/200\n",
            " - 0s - loss: 6.3445 - mae: 1.6352 - val_loss: 7.4360 - val_mae: 1.8961\n",
            "Epoch 48/200\n",
            " - 0s - loss: 6.2340 - mae: 1.6610 - val_loss: 7.4803 - val_mae: 1.8577\n",
            "Epoch 49/200\n",
            " - 0s - loss: 6.0470 - mae: 1.6473 - val_loss: 7.4644 - val_mae: 1.7990\n",
            "Epoch 50/200\n",
            " - 0s - loss: 6.0701 - mae: 1.6382 - val_loss: 7.7347 - val_mae: 1.8431\n",
            "Epoch 51/200\n",
            " - 0s - loss: 5.9401 - mae: 1.6392 - val_loss: 7.2591 - val_mae: 1.9157\n",
            "Epoch 52/200\n",
            " - 0s - loss: 5.7451 - mae: 1.6246 - val_loss: 7.7324 - val_mae: 1.9897\n",
            "Epoch 53/200\n",
            " - 0s - loss: 5.9734 - mae: 1.5740 - val_loss: 7.4943 - val_mae: 1.9976\n",
            "Epoch 54/200\n",
            " - 0s - loss: 5.7558 - mae: 1.5186 - val_loss: 7.5717 - val_mae: 1.9691\n",
            "Epoch 55/200\n",
            " - 0s - loss: 5.5964 - mae: 1.5845 - val_loss: 8.7749 - val_mae: 2.2195\n",
            "Epoch 56/200\n",
            " - 0s - loss: 5.3074 - mae: 1.5682 - val_loss: 7.7383 - val_mae: 2.0191\n",
            "Epoch 57/200\n",
            " - 0s - loss: 5.1664 - mae: 1.5541 - val_loss: 11.1434 - val_mae: 2.5283\n",
            "Epoch 58/200\n",
            " - 0s - loss: 5.5912 - mae: 1.5902 - val_loss: 7.6653 - val_mae: 1.9752\n",
            "Epoch 59/200\n",
            " - 0s - loss: 5.4621 - mae: 1.5224 - val_loss: 7.8642 - val_mae: 2.0831\n",
            "Epoch 60/200\n",
            " - 0s - loss: 5.1742 - mae: 1.5440 - val_loss: 8.1805 - val_mae: 2.0659\n",
            "Epoch 61/200\n",
            " - 0s - loss: 5.2059 - mae: 1.5628 - val_loss: 7.2910 - val_mae: 1.9530\n",
            "Epoch 62/200\n",
            " - 0s - loss: 5.4858 - mae: 1.5800 - val_loss: 8.7367 - val_mae: 2.2018\n",
            "Epoch 63/200\n",
            " - 0s - loss: 5.1630 - mae: 1.5271 - val_loss: 8.0814 - val_mae: 2.0077\n",
            "Epoch 64/200\n",
            " - 0s - loss: 4.9962 - mae: 1.5052 - val_loss: 7.3857 - val_mae: 2.1147\n",
            "Epoch 65/200\n",
            " - 0s - loss: 5.0641 - mae: 1.4878 - val_loss: 7.4425 - val_mae: 2.0143\n",
            "Epoch 66/200\n",
            " - 0s - loss: 5.0782 - mae: 1.5186 - val_loss: 7.6021 - val_mae: 2.0321\n",
            "Epoch 67/200\n",
            " - 0s - loss: 4.6430 - mae: 1.4885 - val_loss: 8.0215 - val_mae: 1.9991\n",
            "Epoch 68/200\n",
            " - 0s - loss: 4.8617 - mae: 1.4954 - val_loss: 8.8593 - val_mae: 2.2309\n",
            "Epoch 69/200\n",
            " - 0s - loss: 4.9590 - mae: 1.4949 - val_loss: 7.4131 - val_mae: 2.0160\n",
            "Epoch 70/200\n",
            " - 0s - loss: 5.0074 - mae: 1.4991 - val_loss: 7.6496 - val_mae: 2.0120\n",
            "Epoch 71/200\n",
            " - 0s - loss: 4.8502 - mae: 1.4991 - val_loss: 8.2718 - val_mae: 2.0133\n",
            "Epoch 72/200\n",
            " - 0s - loss: 4.7051 - mae: 1.4835 - val_loss: 9.2181 - val_mae: 2.1856\n",
            "Epoch 73/200\n",
            " - 0s - loss: 4.3417 - mae: 1.4307 - val_loss: 8.5798 - val_mae: 2.0885\n",
            "Epoch 74/200\n",
            " - 1s - loss: 4.8797 - mae: 1.4971 - val_loss: 8.2803 - val_mae: 2.1044\n",
            "Epoch 75/200\n",
            " - 1s - loss: 4.8606 - mae: 1.4389 - val_loss: 8.2987 - val_mae: 2.2206\n",
            "Epoch 76/200\n",
            " - 1s - loss: 4.6261 - mae: 1.4649 - val_loss: 7.2587 - val_mae: 1.9551\n",
            "Epoch 77/200\n",
            " - 0s - loss: 4.6409 - mae: 1.4740 - val_loss: 7.3673 - val_mae: 1.9996\n",
            "Epoch 78/200\n",
            " - 0s - loss: 4.8088 - mae: 1.4100 - val_loss: 7.5983 - val_mae: 1.9434\n",
            "Epoch 79/200\n",
            " - 0s - loss: 4.5304 - mae: 1.4839 - val_loss: 8.1167 - val_mae: 2.1479\n",
            "Epoch 80/200\n",
            " - 0s - loss: 4.0609 - mae: 1.4027 - val_loss: 7.8511 - val_mae: 2.1156\n",
            "Epoch 81/200\n",
            " - 0s - loss: 4.0368 - mae: 1.4062 - val_loss: 8.4913 - val_mae: 2.0294\n",
            "Epoch 82/200\n",
            " - 0s - loss: 4.2132 - mae: 1.3857 - val_loss: 7.4224 - val_mae: 1.9727\n",
            "Epoch 83/200\n",
            " - 0s - loss: 4.2391 - mae: 1.4101 - val_loss: 8.4458 - val_mae: 2.1349\n",
            "Epoch 84/200\n",
            " - 0s - loss: 4.1469 - mae: 1.3873 - val_loss: 8.5098 - val_mae: 2.2530\n",
            "Epoch 85/200\n",
            " - 0s - loss: 4.2530 - mae: 1.4264 - val_loss: 8.3190 - val_mae: 2.1642\n",
            "Epoch 86/200\n",
            " - 0s - loss: 4.0145 - mae: 1.3413 - val_loss: 8.5709 - val_mae: 2.0746\n",
            "Epoch 87/200\n",
            " - 0s - loss: 4.2233 - mae: 1.3808 - val_loss: 8.1325 - val_mae: 2.2015\n",
            "Epoch 88/200\n",
            " - 0s - loss: 4.1652 - mae: 1.3308 - val_loss: 7.9358 - val_mae: 2.1568\n",
            "Epoch 89/200\n",
            " - 0s - loss: 4.1339 - mae: 1.4022 - val_loss: 7.6606 - val_mae: 1.9964\n",
            "Epoch 90/200\n",
            " - 0s - loss: 3.8095 - mae: 1.3348 - val_loss: 7.5316 - val_mae: 2.1871\n",
            "Epoch 91/200\n",
            " - 0s - loss: 4.0228 - mae: 1.3331 - val_loss: 8.3362 - val_mae: 2.1746\n",
            "Epoch 92/200\n",
            " - 0s - loss: 4.4333 - mae: 1.4309 - val_loss: 9.2232 - val_mae: 2.3034\n",
            "Epoch 93/200\n",
            " - 0s - loss: 3.9442 - mae: 1.3856 - val_loss: 9.5996 - val_mae: 2.2201\n",
            "Epoch 94/200\n",
            " - 0s - loss: 4.0380 - mae: 1.3649 - val_loss: 8.5564 - val_mae: 2.2865\n",
            "Epoch 95/200\n",
            " - 0s - loss: 3.9089 - mae: 1.3473 - val_loss: 8.2402 - val_mae: 2.1047\n",
            "Epoch 96/200\n",
            " - 0s - loss: 4.0264 - mae: 1.3676 - val_loss: 8.5991 - val_mae: 2.1478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 97/200\n",
            " - 0s - loss: 3.8981 - mae: 1.3467 - val_loss: 8.9346 - val_mae: 2.1062\n",
            "Epoch 98/200\n",
            " - 0s - loss: 3.7373 - mae: 1.3036 - val_loss: 9.1716 - val_mae: 2.1701\n",
            "Epoch 99/200\n",
            " - 0s - loss: 3.8749 - mae: 1.3797 - val_loss: 9.3561 - val_mae: 2.2625\n",
            "Epoch 100/200\n",
            " - 0s - loss: 3.7635 - mae: 1.3344 - val_loss: 10.4435 - val_mae: 2.4523\n",
            "Epoch 101/200\n",
            " - 0s - loss: 3.3873 - mae: 1.3024 - val_loss: 9.8928 - val_mae: 2.3630\n",
            "Epoch 102/200\n",
            " - 0s - loss: 3.7631 - mae: 1.3708 - val_loss: 8.8859 - val_mae: 2.1247\n",
            "Epoch 103/200\n",
            " - 0s - loss: 3.6188 - mae: 1.3526 - val_loss: 8.4137 - val_mae: 2.0886\n",
            "Epoch 104/200\n",
            " - 0s - loss: 3.4709 - mae: 1.2854 - val_loss: 10.7329 - val_mae: 2.4514\n",
            "Epoch 105/200\n",
            " - 0s - loss: 3.4639 - mae: 1.2620 - val_loss: 9.5321 - val_mae: 2.2673\n",
            "Epoch 106/200\n",
            " - 0s - loss: 3.7397 - mae: 1.3374 - val_loss: 10.5206 - val_mae: 2.4046\n",
            "Epoch 107/200\n",
            " - 0s - loss: 3.8444 - mae: 1.3494 - val_loss: 9.2701 - val_mae: 2.3056\n",
            "Epoch 108/200\n",
            " - 0s - loss: 3.3535 - mae: 1.2608 - val_loss: 11.0501 - val_mae: 2.5755\n",
            "Epoch 109/200\n",
            " - 0s - loss: 3.5733 - mae: 1.3366 - val_loss: 9.5765 - val_mae: 2.2534\n",
            "Epoch 110/200\n",
            " - 1s - loss: 3.4904 - mae: 1.3178 - val_loss: 10.4013 - val_mae: 2.3708\n",
            "Epoch 111/200\n",
            " - 1s - loss: 3.4582 - mae: 1.2799 - val_loss: 9.5611 - val_mae: 2.2459\n",
            "Epoch 112/200\n",
            " - 0s - loss: 3.3019 - mae: 1.2991 - val_loss: 9.1981 - val_mae: 2.1581\n",
            "Epoch 113/200\n",
            " - 0s - loss: 3.6592 - mae: 1.3609 - val_loss: 8.6380 - val_mae: 2.1708\n",
            "Epoch 114/200\n",
            " - 0s - loss: 3.4778 - mae: 1.2933 - val_loss: 8.7160 - val_mae: 2.1579\n",
            "Epoch 115/200\n",
            " - 0s - loss: 3.7558 - mae: 1.2763 - val_loss: 7.6726 - val_mae: 2.1035\n",
            "Epoch 116/200\n",
            " - 0s - loss: 3.3702 - mae: 1.2819 - val_loss: 9.4666 - val_mae: 2.1029\n",
            "Epoch 117/200\n",
            " - 0s - loss: 3.3460 - mae: 1.2784 - val_loss: 9.0574 - val_mae: 2.2555\n",
            "Epoch 118/200\n",
            " - 0s - loss: 3.2942 - mae: 1.2437 - val_loss: 8.8297 - val_mae: 2.0385\n",
            "Epoch 119/200\n",
            " - 0s - loss: 3.2174 - mae: 1.2310 - val_loss: 8.4224 - val_mae: 2.1383\n",
            "Epoch 120/200\n",
            " - 0s - loss: 3.2237 - mae: 1.2688 - val_loss: 9.6936 - val_mae: 2.3852\n",
            "Epoch 121/200\n",
            " - 0s - loss: 3.0717 - mae: 1.2102 - val_loss: 9.4532 - val_mae: 2.3014\n",
            "Epoch 122/200\n",
            " - 0s - loss: 3.1478 - mae: 1.2140 - val_loss: 9.6239 - val_mae: 2.2809\n",
            "Epoch 123/200\n",
            " - 0s - loss: 2.9379 - mae: 1.2517 - val_loss: 10.0396 - val_mae: 2.3484\n",
            "Epoch 124/200\n",
            " - 0s - loss: 2.9734 - mae: 1.2196 - val_loss: 10.1857 - val_mae: 2.2007\n",
            "Epoch 125/200\n",
            " - 0s - loss: 3.1263 - mae: 1.2826 - val_loss: 8.7696 - val_mae: 2.2173\n",
            "Epoch 126/200\n",
            " - 1s - loss: 3.0320 - mae: 1.2421 - val_loss: 12.4164 - val_mae: 2.6845\n",
            "Epoch 127/200\n",
            " - 1s - loss: 3.0860 - mae: 1.2576 - val_loss: 9.0299 - val_mae: 2.2015\n",
            "Epoch 128/200\n",
            " - 0s - loss: 3.2151 - mae: 1.2505 - val_loss: 8.7658 - val_mae: 2.1974\n",
            "Epoch 129/200\n",
            " - 0s - loss: 2.9627 - mae: 1.2320 - val_loss: 9.1538 - val_mae: 2.1727\n",
            "Epoch 130/200\n",
            " - 0s - loss: 2.9852 - mae: 1.2331 - val_loss: 8.9509 - val_mae: 2.1352\n",
            "Epoch 131/200\n",
            " - 0s - loss: 2.8778 - mae: 1.2549 - val_loss: 9.1823 - val_mae: 2.2369\n",
            "Epoch 132/200\n",
            " - 0s - loss: 2.9091 - mae: 1.2366 - val_loss: 8.6997 - val_mae: 2.1517\n",
            "Epoch 133/200\n",
            " - 0s - loss: 2.9589 - mae: 1.1909 - val_loss: 11.2001 - val_mae: 2.6711\n",
            "Epoch 134/200\n",
            " - 0s - loss: 2.8342 - mae: 1.2056 - val_loss: 11.9572 - val_mae: 2.5173\n",
            "Epoch 135/200\n",
            " - 0s - loss: 3.4325 - mae: 1.2841 - val_loss: 9.5720 - val_mae: 2.1807\n",
            "Epoch 136/200\n",
            " - 0s - loss: 2.8170 - mae: 1.1961 - val_loss: 10.0432 - val_mae: 2.3158\n",
            "Epoch 137/200\n",
            " - 0s - loss: 3.0181 - mae: 1.2207 - val_loss: 8.7483 - val_mae: 2.2366\n",
            "Epoch 138/200\n",
            " - 0s - loss: 2.6301 - mae: 1.1422 - val_loss: 11.4306 - val_mae: 2.4108\n",
            "Epoch 139/200\n",
            " - 0s - loss: 2.7436 - mae: 1.1571 - val_loss: 10.9413 - val_mae: 2.3509\n",
            "Epoch 140/200\n",
            " - 0s - loss: 2.8360 - mae: 1.2268 - val_loss: 9.8831 - val_mae: 2.4489\n",
            "Epoch 141/200\n",
            " - 0s - loss: 3.0084 - mae: 1.2031 - val_loss: 8.8117 - val_mae: 2.2436\n",
            "Epoch 142/200\n",
            " - 0s - loss: 2.6890 - mae: 1.1803 - val_loss: 10.2284 - val_mae: 2.2578\n",
            "Epoch 143/200\n",
            " - 0s - loss: 3.0746 - mae: 1.2223 - val_loss: 10.1253 - val_mae: 2.3339\n",
            "Epoch 144/200\n",
            " - 0s - loss: 2.6657 - mae: 1.1525 - val_loss: 9.9660 - val_mae: 2.3311\n",
            "Epoch 145/200\n",
            " - 0s - loss: 2.7143 - mae: 1.1769 - val_loss: 11.2374 - val_mae: 2.6123\n",
            "Epoch 146/200\n",
            " - 1s - loss: 2.9089 - mae: 1.2273 - val_loss: 9.9367 - val_mae: 2.3326\n",
            "Epoch 147/200\n",
            " - 1s - loss: 2.6559 - mae: 1.1457 - val_loss: 8.8181 - val_mae: 2.1695\n",
            "Epoch 148/200\n",
            " - 0s - loss: 2.9542 - mae: 1.2019 - val_loss: 10.4434 - val_mae: 2.3805\n",
            "Epoch 149/200\n",
            " - 0s - loss: 2.6549 - mae: 1.1399 - val_loss: 9.9079 - val_mae: 2.2035\n",
            "Epoch 150/200\n",
            " - 0s - loss: 2.7720 - mae: 1.1876 - val_loss: 9.8898 - val_mae: 2.2331\n",
            "Epoch 151/200\n",
            " - 0s - loss: 2.4420 - mae: 1.1503 - val_loss: 11.0909 - val_mae: 2.2951\n",
            "Epoch 152/200\n",
            " - 0s - loss: 2.8670 - mae: 1.1496 - val_loss: 10.6865 - val_mae: 2.5098\n",
            "Epoch 153/200\n",
            " - 0s - loss: 2.9063 - mae: 1.1619 - val_loss: 11.9205 - val_mae: 2.5507\n",
            "Epoch 154/200\n",
            " - 0s - loss: 2.7203 - mae: 1.1372 - val_loss: 10.7397 - val_mae: 2.3751\n",
            "Epoch 155/200\n",
            " - 0s - loss: 2.5334 - mae: 1.1387 - val_loss: 10.2266 - val_mae: 2.3135\n",
            "Epoch 156/200\n",
            " - 0s - loss: 2.6684 - mae: 1.1598 - val_loss: 11.3082 - val_mae: 2.4868\n",
            "Epoch 157/200\n",
            " - 0s - loss: 2.9150 - mae: 1.1553 - val_loss: 11.0137 - val_mae: 2.3650\n",
            "Epoch 158/200\n",
            " - 0s - loss: 2.4407 - mae: 1.1239 - val_loss: 11.9515 - val_mae: 2.4691\n",
            "Epoch 159/200\n",
            " - 0s - loss: 2.3956 - mae: 1.1253 - val_loss: 9.6754 - val_mae: 2.1682\n",
            "Epoch 160/200\n",
            " - 0s - loss: 2.6588 - mae: 1.1601 - val_loss: 9.4308 - val_mae: 2.3717\n",
            "Epoch 161/200\n",
            " - 0s - loss: 2.4673 - mae: 1.1368 - val_loss: 10.5487 - val_mae: 2.3775\n",
            "Epoch 162/200\n",
            " - 0s - loss: 2.5777 - mae: 1.0939 - val_loss: 11.3551 - val_mae: 2.6612\n",
            "Epoch 163/200\n",
            " - 0s - loss: 2.3404 - mae: 1.0969 - val_loss: 9.8760 - val_mae: 2.3117\n",
            "Epoch 164/200\n",
            " - 0s - loss: 2.4547 - mae: 1.1431 - val_loss: 9.2181 - val_mae: 2.2105\n",
            "Epoch 165/200\n",
            " - 0s - loss: 2.5296 - mae: 1.1597 - val_loss: 10.2880 - val_mae: 2.2841\n",
            "Epoch 166/200\n",
            " - 0s - loss: 2.2643 - mae: 1.0942 - val_loss: 11.0972 - val_mae: 2.4116\n",
            "Epoch 167/200\n",
            " - 0s - loss: 2.4813 - mae: 1.1321 - val_loss: 13.6021 - val_mae: 2.7027\n",
            "Epoch 168/200\n",
            " - 0s - loss: 2.6101 - mae: 1.1246 - val_loss: 9.5985 - val_mae: 2.2909\n",
            "Epoch 169/200\n",
            " - 0s - loss: 2.2268 - mae: 1.1079 - val_loss: 10.1031 - val_mae: 2.2868\n",
            "Epoch 170/200\n",
            " - 0s - loss: 2.2898 - mae: 1.0643 - val_loss: 10.2325 - val_mae: 2.3087\n",
            "Epoch 171/200\n",
            " - 0s - loss: 2.2829 - mae: 1.0946 - val_loss: 10.9442 - val_mae: 2.3182\n",
            "Epoch 172/200\n",
            " - 0s - loss: 2.4133 - mae: 1.1433 - val_loss: 11.1373 - val_mae: 2.2896\n",
            "Epoch 173/200\n",
            " - 0s - loss: 2.3733 - mae: 1.1457 - val_loss: 9.8940 - val_mae: 2.3441\n",
            "Epoch 174/200\n",
            " - 0s - loss: 2.2695 - mae: 1.0789 - val_loss: 10.3814 - val_mae: 2.3443\n",
            "Epoch 175/200\n",
            " - 0s - loss: 2.3560 - mae: 1.1158 - val_loss: 10.4023 - val_mae: 2.2998\n",
            "Epoch 176/200\n",
            " - 0s - loss: 2.4290 - mae: 1.1104 - val_loss: 11.5695 - val_mae: 2.7325\n",
            "Epoch 177/200\n",
            " - 0s - loss: 2.3578 - mae: 1.1255 - val_loss: 9.6878 - val_mae: 2.2371\n",
            "Epoch 178/200\n",
            " - 0s - loss: 2.3107 - mae: 1.0939 - val_loss: 10.8909 - val_mae: 2.4119\n",
            "Epoch 179/200\n",
            " - 0s - loss: 2.1145 - mae: 1.0659 - val_loss: 8.9159 - val_mae: 2.2426\n",
            "Epoch 180/200\n",
            " - 0s - loss: 2.2985 - mae: 1.0959 - val_loss: 10.2007 - val_mae: 2.2655\n",
            "Epoch 181/200\n",
            " - 0s - loss: 2.1285 - mae: 1.0546 - val_loss: 10.8543 - val_mae: 2.4726\n",
            "Epoch 182/200\n",
            " - 1s - loss: 2.1899 - mae: 1.1074 - val_loss: 9.4751 - val_mae: 2.3362\n",
            "Epoch 183/200\n",
            " - 0s - loss: 2.1543 - mae: 1.1162 - val_loss: 11.2172 - val_mae: 2.4918\n",
            "Epoch 184/200\n",
            " - 0s - loss: 2.2982 - mae: 1.0799 - val_loss: 9.8574 - val_mae: 2.2582\n",
            "Epoch 185/200\n",
            " - 0s - loss: 2.2883 - mae: 1.1267 - val_loss: 8.9427 - val_mae: 2.1388\n",
            "Epoch 186/200\n",
            " - 0s - loss: 2.3406 - mae: 1.1192 - val_loss: 10.2969 - val_mae: 2.3959\n",
            "Epoch 187/200\n",
            " - 0s - loss: 2.0486 - mae: 0.9942 - val_loss: 10.9325 - val_mae: 2.4464\n",
            "Epoch 188/200\n",
            " - 0s - loss: 2.4224 - mae: 1.0959 - val_loss: 12.0504 - val_mae: 2.5387\n",
            "Epoch 189/200\n",
            " - 0s - loss: 2.2910 - mae: 1.1267 - val_loss: 9.7009 - val_mae: 2.2579\n",
            "Epoch 190/200\n",
            " - 0s - loss: 2.3644 - mae: 1.0877 - val_loss: 9.0813 - val_mae: 2.2316\n",
            "Epoch 191/200\n",
            " - 0s - loss: 2.1500 - mae: 1.0799 - val_loss: 10.6723 - val_mae: 2.5844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 192/200\n",
            " - 0s - loss: 2.1147 - mae: 1.0407 - val_loss: 12.4089 - val_mae: 2.7526\n",
            "Epoch 193/200\n",
            " - 0s - loss: 2.1224 - mae: 1.0922 - val_loss: 10.7320 - val_mae: 2.4594\n",
            "Epoch 194/200\n",
            " - 0s - loss: 1.9785 - mae: 1.0435 - val_loss: 11.9461 - val_mae: 2.4583\n",
            "Epoch 195/200\n",
            " - 0s - loss: 2.0980 - mae: 1.0479 - val_loss: 9.6251 - val_mae: 2.3607\n",
            "Epoch 196/200\n",
            " - 0s - loss: 2.0987 - mae: 1.0091 - val_loss: 10.9403 - val_mae: 2.4208\n",
            "Epoch 197/200\n",
            " - 0s - loss: 2.1451 - mae: 1.0362 - val_loss: 9.2291 - val_mae: 2.3100\n",
            "Epoch 198/200\n",
            " - 0s - loss: 1.9505 - mae: 1.0439 - val_loss: 9.4407 - val_mae: 2.3669\n",
            "Epoch 199/200\n",
            " - 0s - loss: 2.2647 - mae: 1.0873 - val_loss: 10.1454 - val_mae: 2.4622\n",
            "Epoch 200/200\n",
            " - 0s - loss: 2.2629 - mae: 1.1113 - val_loss: 11.3133 - val_mae: 2.4347\n",
            "processing fold # 1\n",
            "Train on 303 samples, validate on 101 samples\n",
            "Epoch 1/200\n",
            " - 1s - loss: 173.2023 - mae: 9.7205 - val_loss: 30.1057 - val_mae: 4.0849\n",
            "Epoch 2/200\n",
            " - 0s - loss: 28.4696 - mae: 3.5091 - val_loss: 19.1309 - val_mae: 3.2421\n",
            "Epoch 3/200\n",
            " - 0s - loss: 20.3123 - mae: 2.9286 - val_loss: 20.4934 - val_mae: 3.3354\n",
            "Epoch 4/200\n",
            " - 0s - loss: 17.1788 - mae: 2.6784 - val_loss: 16.0004 - val_mae: 2.9077\n",
            "Epoch 5/200\n",
            " - 0s - loss: 15.1961 - mae: 2.5034 - val_loss: 17.0688 - val_mae: 3.1279\n",
            "Epoch 6/200\n",
            " - 0s - loss: 13.6647 - mae: 2.5099 - val_loss: 15.6658 - val_mae: 2.9728\n",
            "Epoch 7/200\n",
            " - 0s - loss: 12.4173 - mae: 2.3622 - val_loss: 14.6882 - val_mae: 2.8841\n",
            "Epoch 8/200\n",
            " - 0s - loss: 11.8827 - mae: 2.2799 - val_loss: 12.8489 - val_mae: 2.7327\n",
            "Epoch 9/200\n",
            " - 0s - loss: 11.9825 - mae: 2.2889 - val_loss: 13.8547 - val_mae: 2.8212\n",
            "Epoch 10/200\n",
            " - 0s - loss: 11.1541 - mae: 2.2268 - val_loss: 13.2524 - val_mae: 2.7724\n",
            "Epoch 11/200\n",
            " - 0s - loss: 11.1345 - mae: 2.1473 - val_loss: 14.4071 - val_mae: 2.9347\n",
            "Epoch 12/200\n",
            " - 0s - loss: 10.6962 - mae: 2.1636 - val_loss: 11.6687 - val_mae: 2.5950\n",
            "Epoch 13/200\n",
            " - 0s - loss: 10.1787 - mae: 2.1180 - val_loss: 11.9471 - val_mae: 2.6363\n",
            "Epoch 14/200\n",
            " - 0s - loss: 10.4806 - mae: 2.1060 - val_loss: 10.8706 - val_mae: 2.5006\n",
            "Epoch 15/200\n",
            " - 0s - loss: 10.0741 - mae: 2.0553 - val_loss: 13.0795 - val_mae: 2.7572\n",
            "Epoch 16/200\n",
            " - 1s - loss: 9.9001 - mae: 2.0752 - val_loss: 10.7280 - val_mae: 2.4908\n",
            "Epoch 17/200\n",
            " - 1s - loss: 9.2081 - mae: 2.0161 - val_loss: 13.2814 - val_mae: 2.7887\n",
            "Epoch 18/200\n",
            " - 0s - loss: 9.2013 - mae: 2.0079 - val_loss: 10.6953 - val_mae: 2.5183\n",
            "Epoch 19/200\n",
            " - 0s - loss: 9.4532 - mae: 2.0105 - val_loss: 11.2590 - val_mae: 2.6330\n",
            "Epoch 20/200\n",
            " - 0s - loss: 9.3913 - mae: 2.0371 - val_loss: 10.2117 - val_mae: 2.4768\n",
            "Epoch 21/200\n",
            " - 0s - loss: 9.0703 - mae: 1.9931 - val_loss: 9.6438 - val_mae: 2.3836\n",
            "Epoch 22/200\n",
            " - 0s - loss: 9.3573 - mae: 1.9571 - val_loss: 10.2904 - val_mae: 2.4573\n",
            "Epoch 23/200\n",
            " - 0s - loss: 8.7178 - mae: 1.9364 - val_loss: 9.5783 - val_mae: 2.3597\n",
            "Epoch 24/200\n",
            " - 0s - loss: 8.6362 - mae: 1.9715 - val_loss: 10.1314 - val_mae: 2.4592\n",
            "Epoch 25/200\n",
            " - 0s - loss: 8.9041 - mae: 1.9762 - val_loss: 11.4609 - val_mae: 2.6473\n",
            "Epoch 26/200\n",
            " - 0s - loss: 8.4483 - mae: 1.9411 - val_loss: 13.5216 - val_mae: 2.8366\n",
            "Epoch 27/200\n",
            " - 0s - loss: 8.2480 - mae: 1.9231 - val_loss: 11.2340 - val_mae: 2.5675\n",
            "Epoch 28/200\n",
            " - 0s - loss: 8.1859 - mae: 1.9320 - val_loss: 11.7449 - val_mae: 2.6549\n",
            "Epoch 29/200\n",
            " - 0s - loss: 8.2191 - mae: 1.8794 - val_loss: 9.3356 - val_mae: 2.3303\n",
            "Epoch 30/200\n",
            " - 0s - loss: 8.3362 - mae: 1.8649 - val_loss: 12.1598 - val_mae: 2.7308\n",
            "Epoch 31/200\n",
            " - 0s - loss: 7.8967 - mae: 1.8605 - val_loss: 9.0904 - val_mae: 2.2958\n",
            "Epoch 32/200\n",
            " - 0s - loss: 8.0773 - mae: 1.8712 - val_loss: 10.0361 - val_mae: 2.4133\n",
            "Epoch 33/200\n",
            " - 0s - loss: 8.2119 - mae: 1.8070 - val_loss: 10.2077 - val_mae: 2.4750\n",
            "Epoch 34/200\n",
            " - 0s - loss: 7.8992 - mae: 1.8410 - val_loss: 10.4802 - val_mae: 2.5109\n",
            "Epoch 35/200\n",
            " - 0s - loss: 7.4898 - mae: 1.8051 - val_loss: 8.9714 - val_mae: 2.2783\n",
            "Epoch 36/200\n",
            " - 0s - loss: 7.7118 - mae: 1.8319 - val_loss: 10.0274 - val_mae: 2.4364\n",
            "Epoch 37/200\n",
            " - 0s - loss: 7.4723 - mae: 1.7376 - val_loss: 10.1373 - val_mae: 2.4522\n",
            "Epoch 38/200\n",
            " - 0s - loss: 7.8607 - mae: 1.7924 - val_loss: 9.2866 - val_mae: 2.3576\n",
            "Epoch 39/200\n",
            " - 0s - loss: 7.4258 - mae: 1.8072 - val_loss: 8.4089 - val_mae: 2.2177\n",
            "Epoch 40/200\n",
            " - 0s - loss: 7.2101 - mae: 1.8061 - val_loss: 10.3043 - val_mae: 2.4775\n",
            "Epoch 41/200\n",
            " - 0s - loss: 6.9866 - mae: 1.8016 - val_loss: 8.7720 - val_mae: 2.2459\n",
            "Epoch 42/200\n",
            " - 0s - loss: 7.4579 - mae: 1.7499 - val_loss: 10.5789 - val_mae: 2.4908\n",
            "Epoch 43/200\n",
            " - 0s - loss: 7.3560 - mae: 1.7824 - val_loss: 11.2259 - val_mae: 2.6265\n",
            "Epoch 44/200\n",
            " - 0s - loss: 7.0321 - mae: 1.7164 - val_loss: 11.9186 - val_mae: 2.6112\n",
            "Epoch 45/200\n",
            " - 0s - loss: 7.0494 - mae: 1.7524 - val_loss: 9.9017 - val_mae: 2.4634\n",
            "Epoch 46/200\n",
            " - 0s - loss: 7.0938 - mae: 1.7292 - val_loss: 10.5998 - val_mae: 2.5328\n",
            "Epoch 47/200\n",
            " - 0s - loss: 6.8919 - mae: 1.7563 - val_loss: 9.0825 - val_mae: 2.3183\n",
            "Epoch 48/200\n",
            " - 0s - loss: 6.7282 - mae: 1.6954 - val_loss: 9.0448 - val_mae: 2.3151\n",
            "Epoch 49/200\n",
            " - 0s - loss: 7.0106 - mae: 1.6855 - val_loss: 9.1605 - val_mae: 2.2997\n",
            "Epoch 50/200\n",
            " - 0s - loss: 6.5887 - mae: 1.6478 - val_loss: 8.6394 - val_mae: 2.2616\n",
            "Epoch 51/200\n",
            " - 0s - loss: 6.8629 - mae: 1.6934 - val_loss: 11.5285 - val_mae: 2.6060\n",
            "Epoch 52/200\n",
            " - 0s - loss: 6.7606 - mae: 1.6617 - val_loss: 11.7403 - val_mae: 2.6847\n",
            "Epoch 53/200\n",
            " - 1s - loss: 6.2155 - mae: 1.7366 - val_loss: 10.2544 - val_mae: 2.5060\n",
            "Epoch 54/200\n",
            " - 0s - loss: 6.1572 - mae: 1.6449 - val_loss: 8.8439 - val_mae: 2.2887\n",
            "Epoch 55/200\n",
            " - 0s - loss: 5.6815 - mae: 1.5711 - val_loss: 11.3420 - val_mae: 2.6109\n",
            "Epoch 56/200\n",
            " - 0s - loss: 6.4515 - mae: 1.6256 - val_loss: 10.5179 - val_mae: 2.5020\n",
            "Epoch 57/200\n",
            " - 0s - loss: 6.1461 - mae: 1.6122 - val_loss: 11.3057 - val_mae: 2.6044\n",
            "Epoch 58/200\n",
            " - 0s - loss: 6.1148 - mae: 1.6283 - val_loss: 8.8026 - val_mae: 2.2797\n",
            "Epoch 59/200\n",
            " - 0s - loss: 5.9664 - mae: 1.6141 - val_loss: 10.3717 - val_mae: 2.4368\n",
            "Epoch 60/200\n",
            " - 0s - loss: 6.2423 - mae: 1.6124 - val_loss: 11.8957 - val_mae: 2.6440\n",
            "Epoch 61/200\n",
            " - 0s - loss: 5.7804 - mae: 1.5640 - val_loss: 8.3870 - val_mae: 2.1645\n",
            "Epoch 62/200\n",
            " - 0s - loss: 5.9576 - mae: 1.5428 - val_loss: 9.1338 - val_mae: 2.2604\n",
            "Epoch 63/200\n",
            " - 0s - loss: 5.6570 - mae: 1.5738 - val_loss: 8.9265 - val_mae: 2.2913\n",
            "Epoch 64/200\n",
            " - 0s - loss: 5.3451 - mae: 1.5452 - val_loss: 9.2839 - val_mae: 2.2950\n",
            "Epoch 65/200\n",
            " - 0s - loss: 5.8824 - mae: 1.5464 - val_loss: 10.7819 - val_mae: 2.4968\n",
            "Epoch 66/200\n",
            " - 0s - loss: 5.5194 - mae: 1.5238 - val_loss: 9.7559 - val_mae: 2.3543\n",
            "Epoch 67/200\n",
            " - 0s - loss: 5.6063 - mae: 1.5355 - val_loss: 10.3815 - val_mae: 2.4588\n",
            "Epoch 68/200\n",
            " - 0s - loss: 5.3576 - mae: 1.5446 - val_loss: 10.5859 - val_mae: 2.4771\n",
            "Epoch 69/200\n",
            " - 0s - loss: 5.0432 - mae: 1.4832 - val_loss: 9.9265 - val_mae: 2.3576\n",
            "Epoch 70/200\n",
            " - 0s - loss: 4.9862 - mae: 1.4915 - val_loss: 12.6874 - val_mae: 2.6511\n",
            "Epoch 71/200\n",
            " - 0s - loss: 5.4007 - mae: 1.5720 - val_loss: 8.1754 - val_mae: 2.1974\n",
            "Epoch 72/200\n",
            " - 0s - loss: 5.4630 - mae: 1.5088 - val_loss: 12.0671 - val_mae: 2.6762\n",
            "Epoch 73/200\n",
            " - 0s - loss: 5.2423 - mae: 1.4724 - val_loss: 10.5879 - val_mae: 2.4976\n",
            "Epoch 74/200\n",
            " - 0s - loss: 5.1675 - mae: 1.5301 - val_loss: 9.0012 - val_mae: 2.2454\n",
            "Epoch 75/200\n",
            " - 0s - loss: 4.8774 - mae: 1.4906 - val_loss: 9.9202 - val_mae: 2.3542\n",
            "Epoch 76/200\n",
            " - 0s - loss: 5.1892 - mae: 1.5189 - val_loss: 9.5634 - val_mae: 2.3169\n",
            "Epoch 77/200\n",
            " - 0s - loss: 4.9358 - mae: 1.4318 - val_loss: 10.0169 - val_mae: 2.3657\n",
            "Epoch 78/200\n",
            " - 0s - loss: 5.0515 - mae: 1.4540 - val_loss: 10.5431 - val_mae: 2.4894\n",
            "Epoch 79/200\n",
            " - 0s - loss: 4.6611 - mae: 1.4162 - val_loss: 11.1774 - val_mae: 2.5856\n",
            "Epoch 80/200\n",
            " - 0s - loss: 4.5372 - mae: 1.4667 - val_loss: 9.0468 - val_mae: 2.2827\n",
            "Epoch 81/200\n",
            " - 0s - loss: 4.6810 - mae: 1.4727 - val_loss: 10.2867 - val_mae: 2.4984\n",
            "Epoch 82/200\n",
            " - 0s - loss: 4.6087 - mae: 1.4705 - val_loss: 12.1115 - val_mae: 2.6640\n",
            "Epoch 83/200\n",
            " - 0s - loss: 4.6394 - mae: 1.4115 - val_loss: 10.8993 - val_mae: 2.5086\n",
            "Epoch 84/200\n",
            " - 0s - loss: 4.5690 - mae: 1.4427 - val_loss: 8.7488 - val_mae: 2.1507\n",
            "Epoch 85/200\n",
            " - 0s - loss: 4.2693 - mae: 1.4426 - val_loss: 10.7197 - val_mae: 2.4033\n",
            "Epoch 86/200\n",
            " - 0s - loss: 4.5096 - mae: 1.4167 - val_loss: 11.7304 - val_mae: 2.6237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 87/200\n",
            " - 0s - loss: 4.2696 - mae: 1.3862 - val_loss: 10.0856 - val_mae: 2.3997\n",
            "Epoch 88/200\n",
            " - 0s - loss: 4.3331 - mae: 1.4142 - val_loss: 9.7689 - val_mae: 2.3043\n",
            "Epoch 89/200\n",
            " - 0s - loss: 4.3566 - mae: 1.3584 - val_loss: 12.0766 - val_mae: 2.6253\n",
            "Epoch 90/200\n",
            " - 1s - loss: 4.0308 - mae: 1.3739 - val_loss: 9.2038 - val_mae: 2.1931\n",
            "Epoch 91/200\n",
            " - 0s - loss: 4.2618 - mae: 1.4047 - val_loss: 9.6737 - val_mae: 2.2501\n",
            "Epoch 92/200\n",
            " - 0s - loss: 4.4592 - mae: 1.4029 - val_loss: 10.8047 - val_mae: 2.4261\n",
            "Epoch 93/200\n",
            " - 0s - loss: 4.0299 - mae: 1.3944 - val_loss: 10.6867 - val_mae: 2.4356\n",
            "Epoch 94/200\n",
            " - 0s - loss: 4.0606 - mae: 1.3437 - val_loss: 8.4590 - val_mae: 2.1083\n",
            "Epoch 95/200\n",
            " - 0s - loss: 3.9262 - mae: 1.3097 - val_loss: 9.9661 - val_mae: 2.3909\n",
            "Epoch 96/200\n",
            " - 0s - loss: 3.8002 - mae: 1.3288 - val_loss: 10.3735 - val_mae: 2.5081\n",
            "Epoch 97/200\n",
            " - 0s - loss: 4.0066 - mae: 1.3394 - val_loss: 8.5464 - val_mae: 2.1800\n",
            "Epoch 98/200\n",
            " - 0s - loss: 3.8554 - mae: 1.3612 - val_loss: 9.3319 - val_mae: 2.2736\n",
            "Epoch 99/200\n",
            " - 0s - loss: 3.9243 - mae: 1.3070 - val_loss: 11.7134 - val_mae: 2.5914\n",
            "Epoch 100/200\n",
            " - 0s - loss: 3.9962 - mae: 1.3258 - val_loss: 10.5373 - val_mae: 2.4100\n",
            "Epoch 101/200\n",
            " - 0s - loss: 3.7925 - mae: 1.3539 - val_loss: 9.7493 - val_mae: 2.3158\n",
            "Epoch 102/200\n",
            " - 0s - loss: 3.8120 - mae: 1.3602 - val_loss: 10.3594 - val_mae: 2.4553\n",
            "Epoch 103/200\n",
            " - 0s - loss: 3.9360 - mae: 1.3667 - val_loss: 11.9456 - val_mae: 2.4750\n",
            "Epoch 104/200\n",
            " - 0s - loss: 3.6183 - mae: 1.3157 - val_loss: 11.5641 - val_mae: 2.6574\n",
            "Epoch 105/200\n",
            " - 0s - loss: 3.8487 - mae: 1.3050 - val_loss: 10.8637 - val_mae: 2.5480\n",
            "Epoch 106/200\n",
            " - 0s - loss: 3.6891 - mae: 1.3230 - val_loss: 10.9374 - val_mae: 2.5834\n",
            "Epoch 107/200\n",
            " - 0s - loss: 3.4024 - mae: 1.2677 - val_loss: 16.6361 - val_mae: 3.1144\n",
            "Epoch 108/200\n",
            " - 0s - loss: 3.5374 - mae: 1.2792 - val_loss: 9.0755 - val_mae: 2.2984\n",
            "Epoch 109/200\n",
            " - 0s - loss: 3.6246 - mae: 1.2978 - val_loss: 12.8650 - val_mae: 2.7499\n",
            "Epoch 110/200\n",
            " - 0s - loss: 3.3912 - mae: 1.2318 - val_loss: 15.8150 - val_mae: 3.0550\n",
            "Epoch 111/200\n",
            " - 0s - loss: 3.8024 - mae: 1.3172 - val_loss: 10.5014 - val_mae: 2.4549\n",
            "Epoch 112/200\n",
            " - 0s - loss: 3.4343 - mae: 1.2762 - val_loss: 10.3231 - val_mae: 2.4144\n",
            "Epoch 113/200\n",
            " - 0s - loss: 3.5145 - mae: 1.2938 - val_loss: 11.7803 - val_mae: 2.5259\n",
            "Epoch 114/200\n",
            " - 0s - loss: 3.2999 - mae: 1.2503 - val_loss: 10.5900 - val_mae: 2.4556\n",
            "Epoch 115/200\n",
            " - 0s - loss: 3.5920 - mae: 1.2898 - val_loss: 15.0067 - val_mae: 2.8711\n",
            "Epoch 116/200\n",
            " - 0s - loss: 2.8925 - mae: 1.1668 - val_loss: 11.8979 - val_mae: 2.5950\n",
            "Epoch 117/200\n",
            " - 0s - loss: 3.2449 - mae: 1.2132 - val_loss: 10.6706 - val_mae: 2.4695\n",
            "Epoch 118/200\n",
            " - 0s - loss: 3.1951 - mae: 1.2238 - val_loss: 8.6980 - val_mae: 2.1884\n",
            "Epoch 119/200\n",
            " - 0s - loss: 3.1750 - mae: 1.2228 - val_loss: 10.5083 - val_mae: 2.4256\n",
            "Epoch 120/200\n",
            " - 0s - loss: 3.1133 - mae: 1.1845 - val_loss: 12.5454 - val_mae: 2.7222\n",
            "Epoch 121/200\n",
            " - 0s - loss: 3.3618 - mae: 1.2342 - val_loss: 12.1386 - val_mae: 2.6196\n",
            "Epoch 122/200\n",
            " - 0s - loss: 3.0734 - mae: 1.1697 - val_loss: 13.5927 - val_mae: 2.8136\n",
            "Epoch 123/200\n",
            " - 1s - loss: 3.1463 - mae: 1.2532 - val_loss: 9.2267 - val_mae: 2.2235\n",
            "Epoch 124/200\n",
            " - 0s - loss: 3.0159 - mae: 1.1992 - val_loss: 10.3527 - val_mae: 2.4023\n",
            "Epoch 125/200\n",
            " - 0s - loss: 3.0030 - mae: 1.1958 - val_loss: 10.0421 - val_mae: 2.3226\n",
            "Epoch 126/200\n",
            " - 0s - loss: 2.9733 - mae: 1.1762 - val_loss: 12.5959 - val_mae: 2.5394\n",
            "Epoch 127/200\n",
            " - 1s - loss: 3.1038 - mae: 1.2251 - val_loss: 9.9789 - val_mae: 2.3225\n",
            "Epoch 128/200\n",
            " - 1s - loss: 2.8421 - mae: 1.1187 - val_loss: 11.0512 - val_mae: 2.4808\n",
            "Epoch 129/200\n",
            " - 0s - loss: 3.1427 - mae: 1.2391 - val_loss: 9.0818 - val_mae: 2.2295\n",
            "Epoch 130/200\n",
            " - 0s - loss: 2.9743 - mae: 1.1589 - val_loss: 12.4244 - val_mae: 2.5819\n",
            "Epoch 131/200\n",
            " - 0s - loss: 2.9221 - mae: 1.1533 - val_loss: 11.7365 - val_mae: 2.5434\n",
            "Epoch 132/200\n",
            " - 0s - loss: 2.8962 - mae: 1.1455 - val_loss: 12.4655 - val_mae: 2.6680\n",
            "Epoch 133/200\n",
            " - 0s - loss: 2.8115 - mae: 1.1652 - val_loss: 11.2084 - val_mae: 2.4983\n",
            "Epoch 134/200\n",
            " - 0s - loss: 2.7281 - mae: 1.1421 - val_loss: 12.8540 - val_mae: 2.6778\n",
            "Epoch 135/200\n",
            " - 0s - loss: 2.8035 - mae: 1.1532 - val_loss: 14.4145 - val_mae: 2.8315\n",
            "Epoch 136/200\n",
            " - 0s - loss: 2.7750 - mae: 1.1066 - val_loss: 9.9591 - val_mae: 2.3763\n",
            "Epoch 137/200\n",
            " - 0s - loss: 2.6133 - mae: 1.1250 - val_loss: 10.5345 - val_mae: 2.4886\n",
            "Epoch 138/200\n",
            " - 0s - loss: 2.7926 - mae: 1.1638 - val_loss: 8.7087 - val_mae: 2.1905\n",
            "Epoch 139/200\n",
            " - 0s - loss: 2.7261 - mae: 1.1380 - val_loss: 13.3701 - val_mae: 2.6986\n",
            "Epoch 140/200\n",
            " - 0s - loss: 2.4827 - mae: 1.1045 - val_loss: 12.1277 - val_mae: 2.5973\n",
            "Epoch 141/200\n",
            " - 0s - loss: 2.7787 - mae: 1.1182 - val_loss: 10.3166 - val_mae: 2.4143\n",
            "Epoch 142/200\n",
            " - 0s - loss: 2.4938 - mae: 1.1143 - val_loss: 9.4636 - val_mae: 2.3786\n",
            "Epoch 143/200\n",
            " - 0s - loss: 2.6669 - mae: 1.1323 - val_loss: 9.6886 - val_mae: 2.3032\n",
            "Epoch 144/200\n",
            " - 0s - loss: 2.5287 - mae: 1.0778 - val_loss: 10.2884 - val_mae: 2.3495\n",
            "Epoch 145/200\n",
            " - 0s - loss: 2.7506 - mae: 1.1589 - val_loss: 11.8885 - val_mae: 2.5415\n",
            "Epoch 146/200\n",
            " - 0s - loss: 2.5544 - mae: 1.0668 - val_loss: 12.1021 - val_mae: 2.5687\n",
            "Epoch 147/200\n",
            " - 0s - loss: 2.4358 - mae: 1.1153 - val_loss: 11.2718 - val_mae: 2.5353\n",
            "Epoch 148/200\n",
            " - 0s - loss: 2.5428 - mae: 1.1321 - val_loss: 10.9626 - val_mae: 2.5486\n",
            "Epoch 149/200\n",
            " - 0s - loss: 2.5557 - mae: 1.0915 - val_loss: 17.0708 - val_mae: 3.0924\n",
            "Epoch 150/200\n",
            " - 0s - loss: 2.4587 - mae: 1.1172 - val_loss: 14.3419 - val_mae: 2.8693\n",
            "Epoch 151/200\n",
            " - 0s - loss: 2.5190 - mae: 1.0846 - val_loss: 18.2011 - val_mae: 2.8445\n",
            "Epoch 152/200\n",
            " - 0s - loss: 2.4150 - mae: 1.1002 - val_loss: 12.7795 - val_mae: 2.6949\n",
            "Epoch 153/200\n",
            " - 0s - loss: 2.4414 - mae: 1.0449 - val_loss: 10.7801 - val_mae: 2.4906\n",
            "Epoch 154/200\n",
            " - 0s - loss: 2.6214 - mae: 1.0776 - val_loss: 12.5605 - val_mae: 2.6540\n",
            "Epoch 155/200\n",
            " - 0s - loss: 2.3329 - mae: 1.0848 - val_loss: 11.0474 - val_mae: 2.4042\n",
            "Epoch 156/200\n",
            " - 0s - loss: 2.2366 - mae: 1.0415 - val_loss: 13.5793 - val_mae: 2.7018\n",
            "Epoch 157/200\n",
            " - 0s - loss: 2.2346 - mae: 1.0602 - val_loss: 17.4637 - val_mae: 2.9801\n",
            "Epoch 158/200\n",
            " - 0s - loss: 2.2063 - mae: 1.0678 - val_loss: 22.9011 - val_mae: 3.0512\n",
            "Epoch 159/200\n",
            " - 0s - loss: 2.3858 - mae: 1.0430 - val_loss: 15.9910 - val_mae: 2.7079\n",
            "Epoch 160/200\n",
            " - 0s - loss: 2.1842 - mae: 1.0180 - val_loss: 20.3590 - val_mae: 3.1484\n",
            "Epoch 161/200\n",
            " - 0s - loss: 2.4204 - mae: 1.0780 - val_loss: 14.2348 - val_mae: 2.7097\n",
            "Epoch 162/200\n",
            " - 0s - loss: 2.0725 - mae: 1.0339 - val_loss: 12.4813 - val_mae: 2.6960\n",
            "Epoch 163/200\n",
            " - 0s - loss: 2.3147 - mae: 1.0417 - val_loss: 10.4595 - val_mae: 2.4345\n",
            "Epoch 164/200\n",
            " - 0s - loss: 2.2307 - mae: 1.0420 - val_loss: 18.7128 - val_mae: 2.9991\n",
            "Epoch 165/200\n",
            " - 0s - loss: 2.3277 - mae: 1.0613 - val_loss: 13.4291 - val_mae: 2.7412\n",
            "Epoch 166/200\n",
            " - 0s - loss: 2.0789 - mae: 1.0350 - val_loss: 11.2974 - val_mae: 2.4404\n",
            "Epoch 167/200\n",
            " - 0s - loss: 2.1026 - mae: 1.0305 - val_loss: 16.3515 - val_mae: 2.7857\n",
            "Epoch 168/200\n",
            " - 0s - loss: 2.2377 - mae: 0.9902 - val_loss: 14.9326 - val_mae: 2.8034\n",
            "Epoch 169/200\n",
            " - 0s - loss: 2.1031 - mae: 1.0033 - val_loss: 14.3391 - val_mae: 2.7636\n",
            "Epoch 170/200\n",
            " - 0s - loss: 2.3282 - mae: 1.0652 - val_loss: 13.9144 - val_mae: 2.6310\n",
            "Epoch 171/200\n",
            " - 0s - loss: 1.9800 - mae: 0.9746 - val_loss: 10.9983 - val_mae: 2.4589\n",
            "Epoch 172/200\n",
            " - 0s - loss: 2.1638 - mae: 1.0247 - val_loss: 17.4831 - val_mae: 3.0616\n",
            "Epoch 173/200\n",
            " - 0s - loss: 2.1153 - mae: 0.9944 - val_loss: 11.5583 - val_mae: 2.5817\n",
            "Epoch 174/200\n",
            " - 0s - loss: 1.9885 - mae: 0.9902 - val_loss: 11.9759 - val_mae: 2.5110\n",
            "Epoch 175/200\n",
            " - 0s - loss: 1.8841 - mae: 1.0057 - val_loss: 12.1699 - val_mae: 2.5457\n",
            "Epoch 176/200\n",
            " - 0s - loss: 2.1059 - mae: 1.0569 - val_loss: 10.3477 - val_mae: 2.4233\n",
            "Epoch 177/200\n",
            " - 0s - loss: 1.9855 - mae: 1.0256 - val_loss: 14.9516 - val_mae: 2.8902\n",
            "Epoch 178/200\n",
            " - 0s - loss: 2.0972 - mae: 1.0083 - val_loss: 14.9218 - val_mae: 2.7132\n",
            "Epoch 179/200\n",
            " - 0s - loss: 1.9201 - mae: 0.9788 - val_loss: 13.0460 - val_mae: 2.6320\n",
            "Epoch 180/200\n",
            " - 0s - loss: 1.7133 - mae: 0.9579 - val_loss: 11.0884 - val_mae: 2.4681\n",
            "Epoch 181/200\n",
            " - 0s - loss: 1.9897 - mae: 1.0235 - val_loss: 12.2368 - val_mae: 2.6127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 182/200\n",
            " - 0s - loss: 1.8409 - mae: 0.9783 - val_loss: 14.8694 - val_mae: 2.7550\n",
            "Epoch 183/200\n",
            " - 0s - loss: 1.7424 - mae: 0.9622 - val_loss: 13.8917 - val_mae: 2.6688\n",
            "Epoch 184/200\n",
            " - 0s - loss: 1.6913 - mae: 0.9639 - val_loss: 14.3497 - val_mae: 2.7934\n",
            "Epoch 185/200\n",
            " - 0s - loss: 2.0208 - mae: 0.9869 - val_loss: 16.3533 - val_mae: 2.8183\n",
            "Epoch 186/200\n",
            " - 0s - loss: 2.0109 - mae: 0.9929 - val_loss: 12.2673 - val_mae: 2.5545\n",
            "Epoch 187/200\n",
            " - 0s - loss: 1.8117 - mae: 0.9521 - val_loss: 11.7793 - val_mae: 2.4148\n",
            "Epoch 188/200\n",
            " - 0s - loss: 2.0016 - mae: 1.0147 - val_loss: 15.3515 - val_mae: 2.8096\n",
            "Epoch 189/200\n",
            " - 0s - loss: 1.7440 - mae: 0.9421 - val_loss: 14.7196 - val_mae: 2.7361\n",
            "Epoch 190/200\n",
            " - 0s - loss: 1.7491 - mae: 0.9676 - val_loss: 23.3309 - val_mae: 3.6202\n",
            "Epoch 191/200\n",
            " - 0s - loss: 1.7458 - mae: 0.9844 - val_loss: 18.7251 - val_mae: 3.0125\n",
            "Epoch 192/200\n",
            " - 0s - loss: 1.8468 - mae: 1.0137 - val_loss: 11.8182 - val_mae: 2.4242\n",
            "Epoch 193/200\n",
            " - 0s - loss: 1.7428 - mae: 0.9617 - val_loss: 12.1351 - val_mae: 2.5066\n",
            "Epoch 194/200\n",
            " - 0s - loss: 1.7761 - mae: 0.9336 - val_loss: 13.7291 - val_mae: 2.6640\n",
            "Epoch 195/200\n",
            " - 0s - loss: 1.7004 - mae: 0.9362 - val_loss: 14.4065 - val_mae: 2.7558\n",
            "Epoch 196/200\n",
            " - 0s - loss: 1.6707 - mae: 0.9448 - val_loss: 11.7010 - val_mae: 2.5302\n",
            "Epoch 197/200\n",
            " - 0s - loss: 1.8441 - mae: 1.0050 - val_loss: 15.1657 - val_mae: 2.6696\n",
            "Epoch 198/200\n",
            " - 0s - loss: 1.7187 - mae: 0.9282 - val_loss: 20.0284 - val_mae: 3.0822\n",
            "Epoch 199/200\n",
            " - 1s - loss: 1.8666 - mae: 0.9825 - val_loss: 14.0528 - val_mae: 2.6607\n",
            "Epoch 200/200\n",
            " - 1s - loss: 1.6033 - mae: 0.9391 - val_loss: 15.1058 - val_mae: 2.7828\n",
            "processing fold # 2\n",
            "Train on 303 samples, validate on 101 samples\n",
            "Epoch 1/200\n",
            " - 1s - loss: 267.4708 - mae: 12.6068 - val_loss: 39.5366 - val_mae: 4.4251\n",
            "Epoch 2/200\n",
            " - 0s - loss: 33.4740 - mae: 3.9228 - val_loss: 23.1297 - val_mae: 3.1287\n",
            "Epoch 3/200\n",
            " - 0s - loss: 20.0206 - mae: 2.9605 - val_loss: 20.3544 - val_mae: 2.8805\n",
            "Epoch 4/200\n",
            " - 0s - loss: 16.4956 - mae: 2.7380 - val_loss: 19.1965 - val_mae: 2.9581\n",
            "Epoch 5/200\n",
            " - 0s - loss: 14.5602 - mae: 2.5526 - val_loss: 19.4297 - val_mae: 2.8805\n",
            "Epoch 6/200\n",
            " - 0s - loss: 12.9251 - mae: 2.4588 - val_loss: 18.5195 - val_mae: 2.7777\n",
            "Epoch 7/200\n",
            " - 0s - loss: 11.9684 - mae: 2.3683 - val_loss: 17.9533 - val_mae: 2.8743\n",
            "Epoch 8/200\n",
            " - 0s - loss: 10.6941 - mae: 2.2563 - val_loss: 17.1922 - val_mae: 2.8598\n",
            "Epoch 9/200\n",
            " - 0s - loss: 10.5678 - mae: 2.2327 - val_loss: 16.4978 - val_mae: 2.8059\n",
            "Epoch 10/200\n",
            " - 0s - loss: 10.0673 - mae: 2.2228 - val_loss: 15.0675 - val_mae: 2.5235\n",
            "Epoch 11/200\n",
            " - 0s - loss: 10.0804 - mae: 2.1775 - val_loss: 14.9297 - val_mae: 2.5254\n",
            "Epoch 12/200\n",
            " - 0s - loss: 9.4872 - mae: 2.1240 - val_loss: 15.3121 - val_mae: 2.6351\n",
            "Epoch 13/200\n",
            " - 0s - loss: 8.9424 - mae: 2.0864 - val_loss: 16.6867 - val_mae: 2.7878\n",
            "Epoch 14/200\n",
            " - 0s - loss: 8.9410 - mae: 2.0729 - val_loss: 16.5731 - val_mae: 2.8881\n",
            "Epoch 15/200\n",
            " - 0s - loss: 8.7767 - mae: 2.0654 - val_loss: 14.3955 - val_mae: 2.4585\n",
            "Epoch 16/200\n",
            " - 0s - loss: 8.8405 - mae: 2.0597 - val_loss: 14.2632 - val_mae: 2.5531\n",
            "Epoch 17/200\n",
            " - 0s - loss: 8.0633 - mae: 1.9794 - val_loss: 14.8717 - val_mae: 2.4940\n",
            "Epoch 18/200\n",
            " - 0s - loss: 8.3707 - mae: 2.0177 - val_loss: 13.8936 - val_mae: 2.4903\n",
            "Epoch 19/200\n",
            " - 0s - loss: 7.7294 - mae: 1.9156 - val_loss: 17.5892 - val_mae: 2.9673\n",
            "Epoch 20/200\n",
            " - 0s - loss: 7.4992 - mae: 1.9260 - val_loss: 17.0236 - val_mae: 2.7959\n",
            "Epoch 21/200\n",
            " - 0s - loss: 7.4625 - mae: 1.9009 - val_loss: 14.7726 - val_mae: 2.6582\n",
            "Epoch 22/200\n",
            " - 0s - loss: 7.0321 - mae: 1.8637 - val_loss: 14.3640 - val_mae: 2.6081\n",
            "Epoch 23/200\n",
            " - 1s - loss: 7.1419 - mae: 1.8853 - val_loss: 14.2161 - val_mae: 2.4783\n",
            "Epoch 24/200\n",
            " - 0s - loss: 6.8925 - mae: 1.7795 - val_loss: 13.7265 - val_mae: 2.4704\n",
            "Epoch 25/200\n",
            " - 0s - loss: 7.0272 - mae: 1.8486 - val_loss: 14.5856 - val_mae: 2.6396\n",
            "Epoch 26/200\n",
            " - 0s - loss: 6.9482 - mae: 1.8296 - val_loss: 14.8554 - val_mae: 2.5900\n",
            "Epoch 27/200\n",
            " - 0s - loss: 6.6858 - mae: 1.8062 - val_loss: 14.8219 - val_mae: 2.5635\n",
            "Epoch 28/200\n",
            " - 0s - loss: 6.5005 - mae: 1.7907 - val_loss: 15.6577 - val_mae: 2.7600\n",
            "Epoch 29/200\n",
            " - 0s - loss: 6.5839 - mae: 1.7254 - val_loss: 13.8791 - val_mae: 2.5412\n",
            "Epoch 30/200\n",
            " - 0s - loss: 6.2935 - mae: 1.7527 - val_loss: 16.3937 - val_mae: 2.7850\n",
            "Epoch 31/200\n",
            " - 0s - loss: 6.5602 - mae: 1.7791 - val_loss: 13.8255 - val_mae: 2.5487\n",
            "Epoch 32/200\n",
            " - 0s - loss: 6.3367 - mae: 1.7720 - val_loss: 15.3354 - val_mae: 2.6972\n",
            "Epoch 33/200\n",
            " - 0s - loss: 6.3086 - mae: 1.6829 - val_loss: 14.9409 - val_mae: 2.5953\n",
            "Epoch 34/200\n",
            " - 1s - loss: 6.1404 - mae: 1.6646 - val_loss: 14.9130 - val_mae: 2.6169\n",
            "Epoch 35/200\n",
            " - 1s - loss: 6.1688 - mae: 1.6846 - val_loss: 13.6320 - val_mae: 2.4475\n",
            "Epoch 36/200\n",
            " - 0s - loss: 5.7612 - mae: 1.6583 - val_loss: 15.0251 - val_mae: 2.6548\n",
            "Epoch 37/200\n",
            " - 0s - loss: 6.0029 - mae: 1.6925 - val_loss: 15.3471 - val_mae: 2.6317\n",
            "Epoch 38/200\n",
            " - 0s - loss: 5.9977 - mae: 1.6613 - val_loss: 14.2904 - val_mae: 2.4965\n",
            "Epoch 39/200\n",
            " - 0s - loss: 5.8496 - mae: 1.6918 - val_loss: 15.3088 - val_mae: 2.5738\n",
            "Epoch 40/200\n",
            " - 0s - loss: 6.0085 - mae: 1.6570 - val_loss: 14.7187 - val_mae: 2.6379\n",
            "Epoch 41/200\n",
            " - 0s - loss: 5.7296 - mae: 1.6237 - val_loss: 14.9901 - val_mae: 2.6537\n",
            "Epoch 42/200\n",
            " - 0s - loss: 5.6451 - mae: 1.6325 - val_loss: 15.7074 - val_mae: 2.5504\n",
            "Epoch 43/200\n",
            " - 0s - loss: 5.6579 - mae: 1.6289 - val_loss: 14.2545 - val_mae: 2.5418\n",
            "Epoch 44/200\n",
            " - 0s - loss: 5.4635 - mae: 1.5730 - val_loss: 16.0104 - val_mae: 2.7800\n",
            "Epoch 45/200\n",
            " - 0s - loss: 5.2215 - mae: 1.5808 - val_loss: 14.6877 - val_mae: 2.5806\n",
            "Epoch 46/200\n",
            " - 0s - loss: 5.3351 - mae: 1.6080 - val_loss: 15.3599 - val_mae: 2.6415\n",
            "Epoch 47/200\n",
            " - 0s - loss: 4.8240 - mae: 1.5457 - val_loss: 15.8454 - val_mae: 2.7315\n",
            "Epoch 48/200\n",
            " - 0s - loss: 5.4734 - mae: 1.5783 - val_loss: 15.6926 - val_mae: 2.6954\n",
            "Epoch 49/200\n",
            " - 0s - loss: 4.7869 - mae: 1.4814 - val_loss: 15.5159 - val_mae: 2.5891\n",
            "Epoch 50/200\n",
            " - 0s - loss: 5.3195 - mae: 1.5270 - val_loss: 15.4423 - val_mae: 2.6697\n",
            "Epoch 51/200\n",
            " - 0s - loss: 5.0567 - mae: 1.5278 - val_loss: 15.0766 - val_mae: 2.6120\n",
            "Epoch 52/200\n",
            " - 0s - loss: 4.7821 - mae: 1.5608 - val_loss: 16.0213 - val_mae: 2.7032\n",
            "Epoch 53/200\n",
            " - 0s - loss: 4.9731 - mae: 1.5152 - val_loss: 16.7315 - val_mae: 2.8780\n",
            "Epoch 54/200\n",
            " - 0s - loss: 4.6528 - mae: 1.4974 - val_loss: 16.1809 - val_mae: 2.8052\n",
            "Epoch 55/200\n",
            " - 0s - loss: 5.1182 - mae: 1.5634 - val_loss: 16.3777 - val_mae: 2.7731\n",
            "Epoch 56/200\n",
            " - 0s - loss: 4.3434 - mae: 1.4557 - val_loss: 15.8906 - val_mae: 2.6510\n",
            "Epoch 57/200\n",
            " - 0s - loss: 4.4263 - mae: 1.4700 - val_loss: 16.1775 - val_mae: 2.7406\n",
            "Epoch 58/200\n",
            " - 0s - loss: 4.6679 - mae: 1.4725 - val_loss: 16.5277 - val_mae: 2.7417\n",
            "Epoch 59/200\n",
            " - 0s - loss: 4.4166 - mae: 1.3922 - val_loss: 14.3048 - val_mae: 2.4653\n",
            "Epoch 60/200\n",
            " - 0s - loss: 4.6360 - mae: 1.4699 - val_loss: 16.3714 - val_mae: 2.7980\n",
            "Epoch 61/200\n",
            " - 0s - loss: 4.6490 - mae: 1.4731 - val_loss: 15.7023 - val_mae: 2.5692\n",
            "Epoch 62/200\n",
            " - 0s - loss: 4.4629 - mae: 1.3734 - val_loss: 15.6906 - val_mae: 2.5930\n",
            "Epoch 63/200\n",
            " - 0s - loss: 4.3100 - mae: 1.4234 - val_loss: 14.1529 - val_mae: 2.5670\n",
            "Epoch 64/200\n",
            " - 0s - loss: 4.0591 - mae: 1.4215 - val_loss: 17.9548 - val_mae: 3.0377\n",
            "Epoch 65/200\n",
            " - 0s - loss: 4.0675 - mae: 1.4045 - val_loss: 18.9969 - val_mae: 3.0840\n",
            "Epoch 66/200\n",
            " - 0s - loss: 4.2005 - mae: 1.4451 - val_loss: 15.5203 - val_mae: 2.6437\n",
            "Epoch 67/200\n",
            " - 0s - loss: 3.9123 - mae: 1.3129 - val_loss: 16.6293 - val_mae: 2.7875\n",
            "Epoch 68/200\n",
            " - 0s - loss: 4.1415 - mae: 1.3678 - val_loss: 15.2506 - val_mae: 2.7634\n",
            "Epoch 69/200\n",
            " - 0s - loss: 4.2062 - mae: 1.4110 - val_loss: 16.7595 - val_mae: 2.7176\n",
            "Epoch 70/200\n",
            " - 0s - loss: 3.7348 - mae: 1.3603 - val_loss: 15.2336 - val_mae: 2.5813\n",
            "Epoch 71/200\n",
            " - 0s - loss: 3.9575 - mae: 1.3619 - val_loss: 14.9900 - val_mae: 2.6424\n",
            "Epoch 72/200\n",
            " - 0s - loss: 3.9179 - mae: 1.3362 - val_loss: 15.3710 - val_mae: 2.6856\n",
            "Epoch 73/200\n",
            " - 0s - loss: 3.8150 - mae: 1.3641 - val_loss: 15.4074 - val_mae: 2.6080\n",
            "Epoch 74/200\n",
            " - 0s - loss: 3.8933 - mae: 1.3420 - val_loss: 18.6303 - val_mae: 3.0712\n",
            "Epoch 75/200\n",
            " - 0s - loss: 3.9607 - mae: 1.3193 - val_loss: 15.0292 - val_mae: 2.6344\n",
            "Epoch 76/200\n",
            " - 0s - loss: 3.5293 - mae: 1.3095 - val_loss: 15.2863 - val_mae: 2.5926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 77/200\n",
            " - 0s - loss: 3.6541 - mae: 1.3392 - val_loss: 14.3023 - val_mae: 2.5095\n",
            "Epoch 78/200\n",
            " - 0s - loss: 3.5021 - mae: 1.2983 - val_loss: 18.5256 - val_mae: 2.8477\n",
            "Epoch 79/200\n",
            " - 0s - loss: 3.5745 - mae: 1.3208 - val_loss: 14.9976 - val_mae: 2.6278\n",
            "Epoch 80/200\n",
            " - 0s - loss: 3.5495 - mae: 1.3019 - val_loss: 15.7043 - val_mae: 2.6720\n",
            "Epoch 81/200\n",
            " - 0s - loss: 3.3402 - mae: 1.3083 - val_loss: 15.2978 - val_mae: 2.6115\n",
            "Epoch 82/200\n",
            " - 0s - loss: 3.5536 - mae: 1.3146 - val_loss: 15.4290 - val_mae: 2.6998\n",
            "Epoch 83/200\n",
            " - 0s - loss: 3.4178 - mae: 1.3144 - val_loss: 13.3200 - val_mae: 2.4039\n",
            "Epoch 84/200\n",
            " - 0s - loss: 3.4778 - mae: 1.3082 - val_loss: 17.5659 - val_mae: 2.9307\n",
            "Epoch 85/200\n",
            " - 0s - loss: 3.6844 - mae: 1.3238 - val_loss: 15.0816 - val_mae: 2.5784\n",
            "Epoch 86/200\n",
            " - 0s - loss: 3.3429 - mae: 1.2170 - val_loss: 16.3775 - val_mae: 2.6890\n",
            "Epoch 87/200\n",
            " - 0s - loss: 3.1386 - mae: 1.2220 - val_loss: 16.6093 - val_mae: 2.8086\n",
            "Epoch 88/200\n",
            " - 1s - loss: 3.3569 - mae: 1.2243 - val_loss: 15.7388 - val_mae: 2.7444\n",
            "Epoch 89/200\n",
            " - 0s - loss: 3.4541 - mae: 1.2461 - val_loss: 14.1521 - val_mae: 2.5913\n",
            "Epoch 90/200\n",
            " - 0s - loss: 3.1397 - mae: 1.2354 - val_loss: 15.4075 - val_mae: 2.7990\n",
            "Epoch 91/200\n",
            " - 0s - loss: 2.9192 - mae: 1.2035 - val_loss: 18.4188 - val_mae: 2.9427\n",
            "Epoch 92/200\n",
            " - 0s - loss: 3.0522 - mae: 1.2476 - val_loss: 13.5403 - val_mae: 2.4664\n",
            "Epoch 93/200\n",
            " - 0s - loss: 3.1114 - mae: 1.2252 - val_loss: 16.7602 - val_mae: 2.9251\n",
            "Epoch 94/200\n",
            " - 0s - loss: 2.8350 - mae: 1.1820 - val_loss: 15.5202 - val_mae: 2.7571\n",
            "Epoch 95/200\n",
            " - 0s - loss: 3.1326 - mae: 1.2171 - val_loss: 16.1680 - val_mae: 2.6901\n",
            "Epoch 96/200\n",
            " - 0s - loss: 3.0307 - mae: 1.2111 - val_loss: 15.0006 - val_mae: 2.6185\n",
            "Epoch 97/200\n",
            " - 0s - loss: 3.1398 - mae: 1.1801 - val_loss: 15.6401 - val_mae: 2.6543\n",
            "Epoch 98/200\n",
            " - 0s - loss: 2.9752 - mae: 1.1538 - val_loss: 13.0720 - val_mae: 2.4675\n",
            "Epoch 99/200\n",
            " - 0s - loss: 2.9562 - mae: 1.2016 - val_loss: 14.9385 - val_mae: 2.6607\n",
            "Epoch 100/200\n",
            " - 0s - loss: 3.0733 - mae: 1.2351 - val_loss: 14.7979 - val_mae: 2.5530\n",
            "Epoch 101/200\n",
            " - 0s - loss: 2.8243 - mae: 1.1516 - val_loss: 16.1205 - val_mae: 2.7052\n",
            "Epoch 102/200\n",
            " - 0s - loss: 2.7767 - mae: 1.1990 - val_loss: 13.9999 - val_mae: 2.5597\n",
            "Epoch 103/200\n",
            " - 0s - loss: 2.8122 - mae: 1.2252 - val_loss: 14.3548 - val_mae: 2.4884\n",
            "Epoch 104/200\n",
            " - 0s - loss: 2.7832 - mae: 1.1777 - val_loss: 14.5604 - val_mae: 2.5939\n",
            "Epoch 105/200\n",
            " - 0s - loss: 3.0876 - mae: 1.1623 - val_loss: 15.2814 - val_mae: 2.6081\n",
            "Epoch 106/200\n",
            " - 0s - loss: 2.7141 - mae: 1.1594 - val_loss: 16.8668 - val_mae: 2.8790\n",
            "Epoch 107/200\n",
            " - 0s - loss: 2.7434 - mae: 1.1678 - val_loss: 17.2048 - val_mae: 2.9797\n",
            "Epoch 108/200\n",
            " - 1s - loss: 2.7366 - mae: 1.1217 - val_loss: 15.3555 - val_mae: 2.6847\n",
            "Epoch 109/200\n",
            " - 1s - loss: 2.6098 - mae: 1.1349 - val_loss: 16.1423 - val_mae: 2.7179\n",
            "Epoch 110/200\n",
            " - 0s - loss: 2.6154 - mae: 1.1241 - val_loss: 14.6538 - val_mae: 2.5596\n",
            "Epoch 111/200\n",
            " - 0s - loss: 2.6478 - mae: 1.0906 - val_loss: 17.2361 - val_mae: 3.0143\n",
            "Epoch 112/200\n",
            " - 0s - loss: 2.4372 - mae: 1.0805 - val_loss: 15.3703 - val_mae: 2.7774\n",
            "Epoch 113/200\n",
            " - 0s - loss: 2.7445 - mae: 1.1389 - val_loss: 14.6462 - val_mae: 2.6358\n",
            "Epoch 114/200\n",
            " - 0s - loss: 2.6972 - mae: 1.1816 - val_loss: 15.4561 - val_mae: 2.6493\n",
            "Epoch 115/200\n",
            " - 0s - loss: 2.4159 - mae: 1.0746 - val_loss: 16.2416 - val_mae: 2.6812\n",
            "Epoch 116/200\n",
            " - 0s - loss: 2.5709 - mae: 1.1297 - val_loss: 14.8431 - val_mae: 2.7002\n",
            "Epoch 117/200\n",
            " - 0s - loss: 2.5354 - mae: 1.0607 - val_loss: 14.3573 - val_mae: 2.6190\n",
            "Epoch 118/200\n",
            " - 0s - loss: 2.5167 - mae: 1.1275 - val_loss: 14.3144 - val_mae: 2.5335\n",
            "Epoch 119/200\n",
            " - 0s - loss: 2.6092 - mae: 1.1020 - val_loss: 15.0136 - val_mae: 2.6413\n",
            "Epoch 120/200\n",
            " - 0s - loss: 2.3740 - mae: 1.0992 - val_loss: 14.3364 - val_mae: 2.5933\n",
            "Epoch 121/200\n",
            " - 0s - loss: 2.3685 - mae: 1.1095 - val_loss: 17.6720 - val_mae: 2.8965\n",
            "Epoch 122/200\n",
            " - 0s - loss: 2.4051 - mae: 1.0846 - val_loss: 15.7037 - val_mae: 2.6364\n",
            "Epoch 123/200\n",
            " - 0s - loss: 2.3026 - mae: 1.1065 - val_loss: 16.0980 - val_mae: 2.7607\n",
            "Epoch 124/200\n",
            " - 0s - loss: 2.3929 - mae: 1.0771 - val_loss: 13.2068 - val_mae: 2.3575\n",
            "Epoch 125/200\n",
            " - 0s - loss: 2.2950 - mae: 1.0348 - val_loss: 13.4711 - val_mae: 2.4386\n",
            "Epoch 126/200\n",
            " - 0s - loss: 2.4250 - mae: 1.1119 - val_loss: 13.8292 - val_mae: 2.5460\n",
            "Epoch 127/200\n",
            " - 0s - loss: 2.2211 - mae: 1.0208 - val_loss: 15.5414 - val_mae: 2.7637\n",
            "Epoch 128/200\n",
            " - 0s - loss: 2.2552 - mae: 1.0555 - val_loss: 14.0718 - val_mae: 2.5542\n",
            "Epoch 129/200\n",
            " - 0s - loss: 2.1999 - mae: 1.0253 - val_loss: 15.4145 - val_mae: 2.7179\n",
            "Epoch 130/200\n",
            " - 1s - loss: 2.1897 - mae: 1.0595 - val_loss: 15.1423 - val_mae: 2.7304\n",
            "Epoch 131/200\n",
            " - 1s - loss: 2.0335 - mae: 0.9996 - val_loss: 13.6267 - val_mae: 2.5109\n",
            "Epoch 132/200\n",
            " - 1s - loss: 2.2831 - mae: 1.0604 - val_loss: 14.3862 - val_mae: 2.6706\n",
            "Epoch 133/200\n",
            " - 1s - loss: 2.1695 - mae: 1.0593 - val_loss: 15.4990 - val_mae: 2.6866\n",
            "Epoch 134/200\n",
            " - 0s - loss: 2.0976 - mae: 1.0358 - val_loss: 14.2589 - val_mae: 2.6656\n",
            "Epoch 135/200\n",
            " - 0s - loss: 2.1578 - mae: 1.0674 - val_loss: 15.6998 - val_mae: 2.7112\n",
            "Epoch 136/200\n",
            " - 0s - loss: 2.0630 - mae: 1.0050 - val_loss: 15.8804 - val_mae: 2.7684\n",
            "Epoch 137/200\n",
            " - 0s - loss: 1.8903 - mae: 0.9881 - val_loss: 13.5771 - val_mae: 2.5091\n",
            "Epoch 138/200\n",
            " - 0s - loss: 2.0414 - mae: 1.0388 - val_loss: 14.8587 - val_mae: 2.6824\n",
            "Epoch 139/200\n",
            " - 0s - loss: 2.0233 - mae: 1.0056 - val_loss: 12.7517 - val_mae: 2.4142\n",
            "Epoch 140/200\n",
            " - 0s - loss: 2.0446 - mae: 1.0147 - val_loss: 15.0422 - val_mae: 2.6144\n",
            "Epoch 141/200\n",
            " - 0s - loss: 1.9702 - mae: 1.0267 - val_loss: 13.1350 - val_mae: 2.5132\n",
            "Epoch 142/200\n",
            " - 0s - loss: 1.9187 - mae: 0.9900 - val_loss: 14.3463 - val_mae: 2.6679\n",
            "Epoch 143/200\n",
            " - 0s - loss: 1.9457 - mae: 0.9972 - val_loss: 13.3693 - val_mae: 2.5829\n",
            "Epoch 144/200\n",
            " - 0s - loss: 1.9787 - mae: 0.9884 - val_loss: 14.3804 - val_mae: 2.6297\n",
            "Epoch 145/200\n",
            " - 0s - loss: 1.9765 - mae: 1.0280 - val_loss: 15.5865 - val_mae: 2.7251\n",
            "Epoch 146/200\n",
            " - 0s - loss: 2.0289 - mae: 1.0396 - val_loss: 15.1008 - val_mae: 2.6509\n",
            "Epoch 147/200\n",
            " - 0s - loss: 1.9268 - mae: 0.9904 - val_loss: 13.3328 - val_mae: 2.5526\n",
            "Epoch 148/200\n",
            " - 0s - loss: 2.0168 - mae: 0.9855 - val_loss: 14.4108 - val_mae: 2.7342\n",
            "Epoch 149/200\n",
            " - 0s - loss: 1.9281 - mae: 1.0056 - val_loss: 12.0497 - val_mae: 2.4653\n",
            "Epoch 150/200\n",
            " - 0s - loss: 1.9560 - mae: 0.9703 - val_loss: 12.0629 - val_mae: 2.4457\n",
            "Epoch 151/200\n",
            " - 0s - loss: 1.8205 - mae: 0.9430 - val_loss: 15.0403 - val_mae: 2.7259\n",
            "Epoch 152/200\n",
            " - 0s - loss: 1.8802 - mae: 0.9832 - val_loss: 14.7253 - val_mae: 2.7500\n",
            "Epoch 153/200\n",
            " - 0s - loss: 1.7083 - mae: 0.9439 - val_loss: 13.3143 - val_mae: 2.5930\n",
            "Epoch 154/200\n",
            " - 0s - loss: 1.7579 - mae: 0.9377 - val_loss: 14.7209 - val_mae: 2.6872\n",
            "Epoch 155/200\n",
            " - 0s - loss: 1.7613 - mae: 0.9428 - val_loss: 15.5982 - val_mae: 2.8332\n",
            "Epoch 156/200\n",
            " - 0s - loss: 1.7744 - mae: 0.9428 - val_loss: 12.7257 - val_mae: 2.5353\n",
            "Epoch 157/200\n",
            " - 0s - loss: 1.7979 - mae: 0.9505 - val_loss: 14.7555 - val_mae: 2.7490\n",
            "Epoch 158/200\n",
            " - 0s - loss: 1.7086 - mae: 0.9461 - val_loss: 13.3512 - val_mae: 2.5911\n",
            "Epoch 159/200\n",
            " - 0s - loss: 1.8779 - mae: 1.0031 - val_loss: 13.6172 - val_mae: 2.5962\n",
            "Epoch 160/200\n",
            " - 0s - loss: 1.7247 - mae: 0.9453 - val_loss: 14.7616 - val_mae: 2.7079\n",
            "Epoch 161/200\n",
            " - 0s - loss: 1.5174 - mae: 0.9027 - val_loss: 13.4153 - val_mae: 2.6402\n",
            "Epoch 162/200\n",
            " - 0s - loss: 1.5553 - mae: 0.9192 - val_loss: 13.0203 - val_mae: 2.5646\n",
            "Epoch 163/200\n",
            " - 0s - loss: 1.7443 - mae: 0.9887 - val_loss: 13.3698 - val_mae: 2.5155\n",
            "Epoch 164/200\n",
            " - 0s - loss: 1.7203 - mae: 0.9487 - val_loss: 12.9783 - val_mae: 2.6432\n",
            "Epoch 165/200\n",
            " - 0s - loss: 1.5612 - mae: 0.9478 - val_loss: 14.7252 - val_mae: 2.7538\n",
            "Epoch 166/200\n",
            " - 0s - loss: 1.6941 - mae: 0.9530 - val_loss: 15.6095 - val_mae: 2.8530\n",
            "Epoch 167/200\n",
            " - 0s - loss: 1.7855 - mae: 0.9580 - val_loss: 14.6805 - val_mae: 2.6952\n",
            "Epoch 168/200\n",
            " - 0s - loss: 1.6170 - mae: 0.9179 - val_loss: 13.7829 - val_mae: 2.6989\n",
            "Epoch 169/200\n",
            " - 0s - loss: 1.7531 - mae: 0.9294 - val_loss: 15.5540 - val_mae: 2.8714\n",
            "Epoch 170/200\n",
            " - 0s - loss: 1.5424 - mae: 0.9011 - val_loss: 13.9220 - val_mae: 2.6238\n",
            "Epoch 171/200\n",
            " - 0s - loss: 1.5615 - mae: 0.9024 - val_loss: 14.8159 - val_mae: 2.7919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 172/200\n",
            " - 0s - loss: 1.7601 - mae: 0.9589 - val_loss: 13.3490 - val_mae: 2.6785\n",
            "Epoch 173/200\n",
            " - 0s - loss: 1.5940 - mae: 0.9291 - val_loss: 14.1037 - val_mae: 2.6897\n",
            "Epoch 174/200\n",
            " - 0s - loss: 1.5490 - mae: 0.9044 - val_loss: 13.2724 - val_mae: 2.5969\n",
            "Epoch 175/200\n",
            " - 0s - loss: 1.3765 - mae: 0.8811 - val_loss: 14.3041 - val_mae: 2.7517\n",
            "Epoch 176/200\n",
            " - 0s - loss: 1.5223 - mae: 0.8911 - val_loss: 16.1496 - val_mae: 2.9804\n",
            "Epoch 177/200\n",
            " - 1s - loss: 1.5004 - mae: 0.9001 - val_loss: 13.8408 - val_mae: 2.6904\n",
            "Epoch 178/200\n",
            " - 1s - loss: 1.3481 - mae: 0.8394 - val_loss: 12.6236 - val_mae: 2.6020\n",
            "Epoch 179/200\n",
            " - 1s - loss: 1.5108 - mae: 0.9022 - val_loss: 14.4284 - val_mae: 2.7174\n",
            "Epoch 180/200\n",
            " - 1s - loss: 1.5731 - mae: 0.8809 - val_loss: 16.4417 - val_mae: 2.9549\n",
            "Epoch 181/200\n",
            " - 0s - loss: 1.3803 - mae: 0.8585 - val_loss: 13.9173 - val_mae: 2.6815\n",
            "Epoch 182/200\n",
            " - 0s - loss: 1.4964 - mae: 0.9116 - val_loss: 15.1329 - val_mae: 2.8083\n",
            "Epoch 183/200\n",
            " - 0s - loss: 1.4856 - mae: 0.9172 - val_loss: 14.5257 - val_mae: 2.7548\n",
            "Epoch 184/200\n",
            " - 0s - loss: 1.5643 - mae: 0.9118 - val_loss: 12.2830 - val_mae: 2.5972\n",
            "Epoch 185/200\n",
            " - 0s - loss: 1.3136 - mae: 0.8657 - val_loss: 15.1893 - val_mae: 2.6989\n",
            "Epoch 186/200\n",
            " - 0s - loss: 1.4910 - mae: 0.9024 - val_loss: 13.8151 - val_mae: 2.6964\n",
            "Epoch 187/200\n",
            " - 0s - loss: 1.3746 - mae: 0.8805 - val_loss: 14.2409 - val_mae: 2.6609\n",
            "Epoch 188/200\n",
            " - 0s - loss: 1.5622 - mae: 0.8978 - val_loss: 13.3726 - val_mae: 2.6207\n",
            "Epoch 189/200\n",
            " - 0s - loss: 1.3050 - mae: 0.8544 - val_loss: 15.0572 - val_mae: 2.7925\n",
            "Epoch 190/200\n",
            " - 0s - loss: 1.4225 - mae: 0.8843 - val_loss: 15.3655 - val_mae: 2.8071\n",
            "Epoch 191/200\n",
            " - 0s - loss: 1.4613 - mae: 0.8765 - val_loss: 14.2067 - val_mae: 2.7196\n",
            "Epoch 192/200\n",
            " - 0s - loss: 1.3423 - mae: 0.8555 - val_loss: 13.9953 - val_mae: 2.7015\n",
            "Epoch 193/200\n",
            " - 0s - loss: 1.6100 - mae: 0.8929 - val_loss: 17.4238 - val_mae: 2.9396\n",
            "Epoch 194/200\n",
            " - 0s - loss: 1.4125 - mae: 0.8525 - val_loss: 13.3316 - val_mae: 2.7378\n",
            "Epoch 195/200\n",
            " - 0s - loss: 1.4151 - mae: 0.8362 - val_loss: 14.2886 - val_mae: 2.6883\n",
            "Epoch 196/200\n",
            " - 0s - loss: 1.5501 - mae: 0.9036 - val_loss: 14.8030 - val_mae: 2.7943\n",
            "Epoch 197/200\n",
            " - 0s - loss: 1.4851 - mae: 0.9109 - val_loss: 15.8193 - val_mae: 2.8824\n",
            "Epoch 198/200\n",
            " - 0s - loss: 1.4485 - mae: 0.8693 - val_loss: 15.1898 - val_mae: 2.8196\n",
            "Epoch 199/200\n",
            " - 0s - loss: 1.3914 - mae: 0.8587 - val_loss: 13.7118 - val_mae: 2.7110\n",
            "Epoch 200/200\n",
            " - 0s - loss: 1.3872 - mae: 0.8676 - val_loss: 13.4763 - val_mae: 2.6637\n",
            "processing fold # 3\n",
            "Train on 303 samples, validate on 101 samples\n",
            "Epoch 1/200\n",
            " - 1s - loss: 190.1411 - mae: 10.2462 - val_loss: 76.7360 - val_mae: 6.1015\n",
            "Epoch 2/200\n",
            " - 0s - loss: 30.8728 - mae: 3.9138 - val_loss: 41.2907 - val_mae: 4.1664\n",
            "Epoch 3/200\n",
            " - 0s - loss: 20.6656 - mae: 3.0970 - val_loss: 30.2139 - val_mae: 3.5496\n",
            "Epoch 4/200\n",
            " - 0s - loss: 16.1249 - mae: 2.7450 - val_loss: 23.4408 - val_mae: 3.2153\n",
            "Epoch 5/200\n",
            " - 0s - loss: 14.3521 - mae: 2.5746 - val_loss: 23.7747 - val_mae: 3.1104\n",
            "Epoch 6/200\n",
            " - 0s - loss: 13.6338 - mae: 2.4404 - val_loss: 23.6913 - val_mae: 3.2555\n",
            "Epoch 7/200\n",
            " - 0s - loss: 12.3329 - mae: 2.3284 - val_loss: 22.4824 - val_mae: 3.0986\n",
            "Epoch 8/200\n",
            " - 0s - loss: 12.1354 - mae: 2.3052 - val_loss: 18.3188 - val_mae: 2.8186\n",
            "Epoch 9/200\n",
            " - 0s - loss: 11.7542 - mae: 2.2096 - val_loss: 17.8641 - val_mae: 2.7480\n",
            "Epoch 10/200\n",
            " - 0s - loss: 11.0199 - mae: 2.2160 - val_loss: 17.6069 - val_mae: 2.7895\n",
            "Epoch 11/200\n",
            " - 1s - loss: 10.2031 - mae: 2.1631 - val_loss: 16.7447 - val_mae: 2.7021\n",
            "Epoch 12/200\n",
            " - 1s - loss: 10.4168 - mae: 2.1591 - val_loss: 16.7373 - val_mae: 2.7488\n",
            "Epoch 13/200\n",
            " - 1s - loss: 10.1341 - mae: 2.0653 - val_loss: 15.6978 - val_mae: 2.6679\n",
            "Epoch 14/200\n",
            " - 0s - loss: 9.8996 - mae: 2.0494 - val_loss: 14.8487 - val_mae: 2.6001\n",
            "Epoch 15/200\n",
            " - 0s - loss: 9.0962 - mae: 1.9452 - val_loss: 14.7618 - val_mae: 2.6417\n",
            "Epoch 16/200\n",
            " - 0s - loss: 9.1805 - mae: 2.0707 - val_loss: 14.3469 - val_mae: 2.5945\n",
            "Epoch 17/200\n",
            " - 0s - loss: 8.9639 - mae: 1.9973 - val_loss: 14.2604 - val_mae: 2.7046\n",
            "Epoch 18/200\n",
            " - 0s - loss: 9.1772 - mae: 1.9848 - val_loss: 16.2080 - val_mae: 2.7190\n",
            "Epoch 19/200\n",
            " - 0s - loss: 9.3417 - mae: 1.9550 - val_loss: 13.6672 - val_mae: 2.5293\n",
            "Epoch 20/200\n",
            " - 0s - loss: 8.5188 - mae: 1.9747 - val_loss: 14.2809 - val_mae: 2.4893\n",
            "Epoch 21/200\n",
            " - 0s - loss: 8.4104 - mae: 1.8947 - val_loss: 13.8477 - val_mae: 2.5841\n",
            "Epoch 22/200\n",
            " - 0s - loss: 8.5346 - mae: 1.9076 - val_loss: 14.0738 - val_mae: 2.5472\n",
            "Epoch 23/200\n",
            " - 0s - loss: 8.4294 - mae: 1.8644 - val_loss: 14.1244 - val_mae: 2.7329\n",
            "Epoch 24/200\n",
            " - 0s - loss: 8.1087 - mae: 1.8445 - val_loss: 15.5815 - val_mae: 2.7036\n",
            "Epoch 25/200\n",
            " - 0s - loss: 8.2034 - mae: 1.8522 - val_loss: 11.7220 - val_mae: 2.4089\n",
            "Epoch 26/200\n",
            " - 0s - loss: 8.0389 - mae: 1.8903 - val_loss: 12.5676 - val_mae: 2.5157\n",
            "Epoch 27/200\n",
            " - 0s - loss: 7.8223 - mae: 1.8239 - val_loss: 11.9880 - val_mae: 2.4096\n",
            "Epoch 28/200\n",
            " - 0s - loss: 7.6888 - mae: 1.8449 - val_loss: 12.6771 - val_mae: 2.4099\n",
            "Epoch 29/200\n",
            " - 0s - loss: 7.4220 - mae: 1.7587 - val_loss: 12.8823 - val_mae: 2.4875\n",
            "Epoch 30/200\n",
            " - 0s - loss: 7.3088 - mae: 1.7758 - val_loss: 13.8328 - val_mae: 2.7192\n",
            "Epoch 31/200\n",
            " - 0s - loss: 6.8574 - mae: 1.7766 - val_loss: 14.4966 - val_mae: 2.6147\n",
            "Epoch 32/200\n",
            " - 0s - loss: 7.8855 - mae: 1.7856 - val_loss: 11.3750 - val_mae: 2.3920\n",
            "Epoch 33/200\n",
            " - 0s - loss: 6.9092 - mae: 1.7762 - val_loss: 11.5606 - val_mae: 2.3896\n",
            "Epoch 34/200\n",
            " - 0s - loss: 7.6051 - mae: 1.7056 - val_loss: 15.2312 - val_mae: 2.9846\n",
            "Epoch 35/200\n",
            " - 0s - loss: 7.4356 - mae: 1.8101 - val_loss: 12.9687 - val_mae: 2.6150\n",
            "Epoch 36/200\n",
            " - 0s - loss: 7.4474 - mae: 1.7178 - val_loss: 12.1335 - val_mae: 2.5128\n",
            "Epoch 37/200\n",
            " - 0s - loss: 6.9026 - mae: 1.7299 - val_loss: 12.9601 - val_mae: 2.6034\n",
            "Epoch 38/200\n",
            " - 0s - loss: 7.0774 - mae: 1.7384 - val_loss: 11.0832 - val_mae: 2.3307\n",
            "Epoch 39/200\n",
            " - 0s - loss: 7.0492 - mae: 1.7367 - val_loss: 11.8946 - val_mae: 2.3654\n",
            "Epoch 40/200\n",
            " - 0s - loss: 6.7770 - mae: 1.6698 - val_loss: 13.1094 - val_mae: 2.5788\n",
            "Epoch 41/200\n",
            " - 0s - loss: 6.8405 - mae: 1.7218 - val_loss: 12.0793 - val_mae: 2.5028\n",
            "Epoch 42/200\n",
            " - 0s - loss: 7.0591 - mae: 1.7287 - val_loss: 11.0018 - val_mae: 2.3501\n",
            "Epoch 43/200\n",
            " - 0s - loss: 6.9022 - mae: 1.7153 - val_loss: 11.3213 - val_mae: 2.3333\n",
            "Epoch 44/200\n",
            " - 0s - loss: 6.5523 - mae: 1.6526 - val_loss: 11.5048 - val_mae: 2.4125\n",
            "Epoch 45/200\n",
            " - 0s - loss: 6.2985 - mae: 1.6304 - val_loss: 12.3000 - val_mae: 2.4384\n",
            "Epoch 46/200\n",
            " - 1s - loss: 6.6347 - mae: 1.6341 - val_loss: 11.1702 - val_mae: 2.3553\n",
            "Epoch 47/200\n",
            " - 1s - loss: 6.3979 - mae: 1.5737 - val_loss: 12.3393 - val_mae: 2.4809\n",
            "Epoch 48/200\n",
            " - 0s - loss: 6.5176 - mae: 1.5919 - val_loss: 13.8562 - val_mae: 2.7477\n",
            "Epoch 49/200\n",
            " - 0s - loss: 6.3293 - mae: 1.6261 - val_loss: 11.6031 - val_mae: 2.4495\n",
            "Epoch 50/200\n",
            " - 0s - loss: 6.2215 - mae: 1.5704 - val_loss: 12.1628 - val_mae: 2.3711\n",
            "Epoch 51/200\n",
            " - 0s - loss: 6.4655 - mae: 1.6195 - val_loss: 10.6562 - val_mae: 2.2727\n",
            "Epoch 52/200\n",
            " - 0s - loss: 6.1694 - mae: 1.5766 - val_loss: 10.4599 - val_mae: 2.1840\n",
            "Epoch 53/200\n",
            " - 0s - loss: 6.0709 - mae: 1.5981 - val_loss: 10.9172 - val_mae: 2.3313\n",
            "Epoch 54/200\n",
            " - 0s - loss: 6.5303 - mae: 1.6038 - val_loss: 11.0755 - val_mae: 2.3492\n",
            "Epoch 55/200\n",
            " - 0s - loss: 5.5994 - mae: 1.5223 - val_loss: 12.8950 - val_mae: 2.6509\n",
            "Epoch 56/200\n",
            " - 0s - loss: 5.9289 - mae: 1.6478 - val_loss: 10.9307 - val_mae: 2.3184\n",
            "Epoch 57/200\n",
            " - 0s - loss: 5.9822 - mae: 1.5500 - val_loss: 10.9806 - val_mae: 2.2901\n",
            "Epoch 58/200\n",
            " - 0s - loss: 5.6198 - mae: 1.5206 - val_loss: 11.0008 - val_mae: 2.3103\n",
            "Epoch 59/200\n",
            " - 0s - loss: 5.8525 - mae: 1.5342 - val_loss: 11.7917 - val_mae: 2.4076\n",
            "Epoch 60/200\n",
            " - 0s - loss: 5.9464 - mae: 1.5782 - val_loss: 10.8736 - val_mae: 2.4030\n",
            "Epoch 61/200\n",
            " - 0s - loss: 5.6572 - mae: 1.5244 - val_loss: 11.5431 - val_mae: 2.3994\n",
            "Epoch 62/200\n",
            " - 0s - loss: 5.6592 - mae: 1.5837 - val_loss: 11.4413 - val_mae: 2.2973\n",
            "Epoch 63/200\n",
            " - 0s - loss: 5.5803 - mae: 1.5816 - val_loss: 10.4149 - val_mae: 2.2133\n",
            "Epoch 64/200\n",
            " - 0s - loss: 5.8283 - mae: 1.5039 - val_loss: 10.5260 - val_mae: 2.2033\n",
            "Epoch 65/200\n",
            " - 0s - loss: 5.4782 - mae: 1.5145 - val_loss: 13.6346 - val_mae: 2.8531\n",
            "Epoch 66/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 0s - loss: 5.6892 - mae: 1.5211 - val_loss: 10.9433 - val_mae: 2.2974\n",
            "Epoch 67/200\n",
            " - 0s - loss: 5.7564 - mae: 1.5084 - val_loss: 11.3768 - val_mae: 2.4059\n",
            "Epoch 68/200\n",
            " - 0s - loss: 5.3217 - mae: 1.5202 - val_loss: 10.5992 - val_mae: 2.3572\n",
            "Epoch 69/200\n",
            " - 0s - loss: 5.5147 - mae: 1.4809 - val_loss: 12.6202 - val_mae: 2.5443\n",
            "Epoch 70/200\n",
            " - 0s - loss: 5.3471 - mae: 1.5111 - val_loss: 10.9532 - val_mae: 2.3505\n",
            "Epoch 71/200\n",
            " - 0s - loss: 5.4695 - mae: 1.4532 - val_loss: 10.4960 - val_mae: 2.2746\n",
            "Epoch 72/200\n",
            " - 0s - loss: 5.3494 - mae: 1.4438 - val_loss: 11.4201 - val_mae: 2.3800\n",
            "Epoch 73/200\n",
            " - 0s - loss: 5.2930 - mae: 1.4203 - val_loss: 10.7890 - val_mae: 2.3330\n",
            "Epoch 74/200\n",
            " - 0s - loss: 5.1290 - mae: 1.4631 - val_loss: 9.3228 - val_mae: 2.1366\n",
            "Epoch 75/200\n",
            " - 0s - loss: 5.0232 - mae: 1.4639 - val_loss: 9.3992 - val_mae: 2.1469\n",
            "Epoch 76/200\n",
            " - 0s - loss: 5.3440 - mae: 1.4703 - val_loss: 11.2713 - val_mae: 2.5039\n",
            "Epoch 77/200\n",
            " - 0s - loss: 5.1442 - mae: 1.4091 - val_loss: 10.2423 - val_mae: 2.2063\n",
            "Epoch 78/200\n",
            " - 0s - loss: 5.4186 - mae: 1.4320 - val_loss: 10.1283 - val_mae: 2.2393\n",
            "Epoch 79/200\n",
            " - 0s - loss: 5.0209 - mae: 1.4830 - val_loss: 9.9275 - val_mae: 2.1935\n",
            "Epoch 80/200\n",
            " - 0s - loss: 5.1930 - mae: 1.4526 - val_loss: 12.4379 - val_mae: 2.5461\n",
            "Epoch 81/200\n",
            " - 0s - loss: 5.1423 - mae: 1.4907 - val_loss: 10.8273 - val_mae: 2.3372\n",
            "Epoch 82/200\n",
            " - 0s - loss: 4.9857 - mae: 1.4259 - val_loss: 10.6041 - val_mae: 2.3156\n",
            "Epoch 83/200\n",
            " - 1s - loss: 4.9046 - mae: 1.4353 - val_loss: 10.8496 - val_mae: 2.2872\n",
            "Epoch 84/200\n",
            " - 0s - loss: 4.7510 - mae: 1.3348 - val_loss: 11.0296 - val_mae: 2.4120\n",
            "Epoch 85/200\n",
            " - 0s - loss: 4.8586 - mae: 1.4196 - val_loss: 10.1553 - val_mae: 2.2938\n",
            "Epoch 86/200\n",
            " - 0s - loss: 4.5253 - mae: 1.4042 - val_loss: 10.4573 - val_mae: 2.3461\n",
            "Epoch 87/200\n",
            " - 0s - loss: 4.7053 - mae: 1.4247 - val_loss: 9.6373 - val_mae: 2.2141\n",
            "Epoch 88/200\n",
            " - 0s - loss: 5.0067 - mae: 1.3978 - val_loss: 13.0027 - val_mae: 2.6220\n",
            "Epoch 89/200\n",
            " - 0s - loss: 4.6917 - mae: 1.4004 - val_loss: 10.2419 - val_mae: 2.2544\n",
            "Epoch 90/200\n",
            " - 0s - loss: 4.9537 - mae: 1.4521 - val_loss: 10.2257 - val_mae: 2.2543\n",
            "Epoch 91/200\n",
            " - 0s - loss: 4.6112 - mae: 1.2522 - val_loss: 10.8395 - val_mae: 2.3685\n",
            "Epoch 92/200\n",
            " - 0s - loss: 4.6114 - mae: 1.4071 - val_loss: 9.6472 - val_mae: 2.1893\n",
            "Epoch 93/200\n",
            " - 0s - loss: 4.4300 - mae: 1.3383 - val_loss: 10.4714 - val_mae: 2.3011\n",
            "Epoch 94/200\n",
            " - 0s - loss: 4.6146 - mae: 1.3487 - val_loss: 10.7796 - val_mae: 2.2446\n",
            "Epoch 95/200\n",
            " - 0s - loss: 4.8719 - mae: 1.3984 - val_loss: 11.1713 - val_mae: 2.4341\n",
            "Epoch 96/200\n",
            " - 0s - loss: 4.6836 - mae: 1.3711 - val_loss: 10.3473 - val_mae: 2.3456\n",
            "Epoch 97/200\n",
            " - 0s - loss: 4.5081 - mae: 1.3407 - val_loss: 10.4907 - val_mae: 2.3592\n",
            "Epoch 98/200\n",
            " - 0s - loss: 4.5430 - mae: 1.3656 - val_loss: 9.4540 - val_mae: 2.2042\n",
            "Epoch 99/200\n",
            " - 0s - loss: 4.2336 - mae: 1.2876 - val_loss: 10.5264 - val_mae: 2.3048\n",
            "Epoch 100/200\n",
            " - 0s - loss: 4.1958 - mae: 1.3104 - val_loss: 12.3630 - val_mae: 2.5893\n",
            "Epoch 101/200\n",
            " - 0s - loss: 4.2308 - mae: 1.3261 - val_loss: 9.6442 - val_mae: 2.2353\n",
            "Epoch 102/200\n",
            " - 0s - loss: 4.5367 - mae: 1.3720 - val_loss: 9.3474 - val_mae: 2.1826\n",
            "Epoch 103/200\n",
            " - 0s - loss: 3.9802 - mae: 1.2815 - val_loss: 10.7243 - val_mae: 2.4163\n",
            "Epoch 104/200\n",
            " - 0s - loss: 4.3912 - mae: 1.3290 - val_loss: 10.0321 - val_mae: 2.3770\n",
            "Epoch 105/200\n",
            " - 0s - loss: 4.2114 - mae: 1.2631 - val_loss: 9.1783 - val_mae: 2.1430\n",
            "Epoch 106/200\n",
            " - 0s - loss: 3.9560 - mae: 1.3382 - val_loss: 9.7698 - val_mae: 2.2489\n",
            "Epoch 107/200\n",
            " - 0s - loss: 4.3972 - mae: 1.2949 - val_loss: 10.6425 - val_mae: 2.3215\n",
            "Epoch 108/200\n",
            " - 0s - loss: 4.1294 - mae: 1.2688 - val_loss: 9.1774 - val_mae: 2.1455\n",
            "Epoch 109/200\n",
            " - 0s - loss: 3.8716 - mae: 1.2878 - val_loss: 11.4284 - val_mae: 2.5201\n",
            "Epoch 110/200\n",
            " - 0s - loss: 4.1123 - mae: 1.2702 - val_loss: 13.9452 - val_mae: 2.8712\n",
            "Epoch 111/200\n",
            " - 0s - loss: 4.1082 - mae: 1.3244 - val_loss: 10.1577 - val_mae: 2.3379\n",
            "Epoch 112/200\n",
            " - 0s - loss: 3.9288 - mae: 1.2352 - val_loss: 9.7680 - val_mae: 2.3164\n",
            "Epoch 113/200\n",
            " - 0s - loss: 3.7561 - mae: 1.2754 - val_loss: 8.9409 - val_mae: 2.1338\n",
            "Epoch 114/200\n",
            " - 0s - loss: 4.0587 - mae: 1.2221 - val_loss: 9.1627 - val_mae: 2.1394\n",
            "Epoch 115/200\n",
            " - 0s - loss: 3.8690 - mae: 1.2334 - val_loss: 9.7644 - val_mae: 2.2394\n",
            "Epoch 116/200\n",
            " - 0s - loss: 4.0209 - mae: 1.2724 - val_loss: 9.0645 - val_mae: 2.1499\n",
            "Epoch 117/200\n",
            " - 0s - loss: 3.8035 - mae: 1.2803 - val_loss: 9.4559 - val_mae: 2.2082\n",
            "Epoch 118/200\n",
            " - 0s - loss: 3.6735 - mae: 1.2454 - val_loss: 9.5891 - val_mae: 2.2318\n",
            "Epoch 119/200\n",
            " - 1s - loss: 3.8605 - mae: 1.2514 - val_loss: 9.1418 - val_mae: 2.1433\n",
            "Epoch 120/200\n",
            " - 0s - loss: 3.3468 - mae: 1.1868 - val_loss: 10.5726 - val_mae: 2.4641\n",
            "Epoch 121/200\n",
            " - 0s - loss: 3.9175 - mae: 1.2377 - val_loss: 9.1108 - val_mae: 2.1298\n",
            "Epoch 122/200\n",
            " - 0s - loss: 3.8483 - mae: 1.2033 - val_loss: 9.8731 - val_mae: 2.2601\n",
            "Epoch 123/200\n",
            " - 0s - loss: 3.7958 - mae: 1.1952 - val_loss: 9.1346 - val_mae: 2.1615\n",
            "Epoch 124/200\n",
            " - 0s - loss: 3.6059 - mae: 1.2231 - val_loss: 12.1094 - val_mae: 2.5455\n",
            "Epoch 125/200\n",
            " - 0s - loss: 3.6518 - mae: 1.2561 - val_loss: 10.0322 - val_mae: 2.3176\n",
            "Epoch 126/200\n",
            " - 0s - loss: 3.6637 - mae: 1.2039 - val_loss: 9.9190 - val_mae: 2.2307\n",
            "Epoch 127/200\n",
            " - 0s - loss: 3.6245 - mae: 1.2210 - val_loss: 11.5018 - val_mae: 2.5832\n",
            "Epoch 128/200\n",
            " - 0s - loss: 3.5574 - mae: 1.2160 - val_loss: 9.8892 - val_mae: 2.3699\n",
            "Epoch 129/200\n",
            " - 0s - loss: 3.5534 - mae: 1.1858 - val_loss: 9.5448 - val_mae: 2.2529\n",
            "Epoch 130/200\n",
            " - 0s - loss: 3.6673 - mae: 1.2262 - val_loss: 9.2763 - val_mae: 2.2057\n",
            "Epoch 131/200\n",
            " - 0s - loss: 3.4786 - mae: 1.1417 - val_loss: 11.0411 - val_mae: 2.4289\n",
            "Epoch 132/200\n",
            " - 0s - loss: 3.6030 - mae: 1.2028 - val_loss: 9.9853 - val_mae: 2.4011\n",
            "Epoch 133/200\n",
            " - 0s - loss: 3.5392 - mae: 1.1876 - val_loss: 10.0345 - val_mae: 2.2716\n",
            "Epoch 134/200\n",
            " - 0s - loss: 3.3937 - mae: 1.2024 - val_loss: 10.4397 - val_mae: 2.2733\n",
            "Epoch 135/200\n",
            " - 0s - loss: 3.5552 - mae: 1.1946 - val_loss: 9.5457 - val_mae: 2.3450\n",
            "Epoch 136/200\n",
            " - 0s - loss: 3.3778 - mae: 1.1671 - val_loss: 9.5365 - val_mae: 2.1895\n",
            "Epoch 137/200\n",
            " - 0s - loss: 3.6091 - mae: 1.2105 - val_loss: 9.8514 - val_mae: 2.2773\n",
            "Epoch 138/200\n",
            " - 0s - loss: 3.5671 - mae: 1.1822 - val_loss: 9.4391 - val_mae: 2.1871\n",
            "Epoch 139/200\n",
            " - 0s - loss: 3.2955 - mae: 1.1646 - val_loss: 9.2716 - val_mae: 2.1875\n",
            "Epoch 140/200\n",
            " - 0s - loss: 3.1594 - mae: 1.1775 - val_loss: 9.3269 - val_mae: 2.1548\n",
            "Epoch 141/200\n",
            " - 0s - loss: 3.5619 - mae: 1.1513 - val_loss: 9.5556 - val_mae: 2.2366\n",
            "Epoch 142/200\n",
            " - 0s - loss: 3.2406 - mae: 1.1262 - val_loss: 12.3816 - val_mae: 2.7261\n",
            "Epoch 143/200\n",
            " - 0s - loss: 3.2924 - mae: 1.1256 - val_loss: 10.0067 - val_mae: 2.3415\n",
            "Epoch 144/200\n",
            " - 0s - loss: 3.1066 - mae: 1.1413 - val_loss: 9.7625 - val_mae: 2.2190\n",
            "Epoch 145/200\n",
            " - 0s - loss: 3.2348 - mae: 1.1012 - val_loss: 9.2290 - val_mae: 2.1885\n",
            "Epoch 146/200\n",
            " - 0s - loss: 3.1337 - mae: 1.1249 - val_loss: 9.7711 - val_mae: 2.2766\n",
            "Epoch 147/200\n",
            " - 0s - loss: 3.2277 - mae: 1.1705 - val_loss: 9.3056 - val_mae: 2.1709\n",
            "Epoch 148/200\n",
            " - 0s - loss: 3.1226 - mae: 1.1569 - val_loss: 9.6119 - val_mae: 2.2318\n",
            "Epoch 149/200\n",
            " - 0s - loss: 2.7957 - mae: 1.0637 - val_loss: 9.7080 - val_mae: 2.1668\n",
            "Epoch 150/200\n",
            " - 0s - loss: 3.2868 - mae: 1.1112 - val_loss: 9.8620 - val_mae: 2.2502\n",
            "Epoch 151/200\n",
            " - 0s - loss: 3.1893 - mae: 1.1375 - val_loss: 10.6011 - val_mae: 2.3820\n",
            "Epoch 152/200\n",
            " - 0s - loss: 2.9816 - mae: 1.1036 - val_loss: 10.9846 - val_mae: 2.4325\n",
            "Epoch 153/200\n",
            " - 0s - loss: 2.8559 - mae: 1.1105 - val_loss: 10.6323 - val_mae: 2.3911\n",
            "Epoch 154/200\n",
            " - 0s - loss: 3.0423 - mae: 1.1240 - val_loss: 9.2956 - val_mae: 2.1495\n",
            "Epoch 155/200\n",
            " - 1s - loss: 2.8459 - mae: 1.1229 - val_loss: 11.5228 - val_mae: 2.5642\n",
            "Epoch 156/200\n",
            " - 1s - loss: 2.8161 - mae: 1.1119 - val_loss: 9.8314 - val_mae: 2.2685\n",
            "Epoch 157/200\n",
            " - 1s - loss: 2.7446 - mae: 1.1025 - val_loss: 9.2092 - val_mae: 2.1906\n",
            "Epoch 158/200\n",
            " - 0s - loss: 2.7991 - mae: 1.1113 - val_loss: 9.1870 - val_mae: 2.2406\n",
            "Epoch 159/200\n",
            " - 0s - loss: 2.8713 - mae: 1.0629 - val_loss: 9.8162 - val_mae: 2.2734\n",
            "Epoch 160/200\n",
            " - 0s - loss: 2.8219 - mae: 1.0728 - val_loss: 10.8315 - val_mae: 2.3927\n",
            "Epoch 161/200\n",
            " - 0s - loss: 2.7624 - mae: 1.0730 - val_loss: 9.5298 - val_mae: 2.2438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 162/200\n",
            " - 0s - loss: 2.6902 - mae: 1.0341 - val_loss: 10.1056 - val_mae: 2.3246\n",
            "Epoch 163/200\n",
            " - 0s - loss: 2.8558 - mae: 1.1246 - val_loss: 9.7417 - val_mae: 2.2581\n",
            "Epoch 164/200\n",
            " - 0s - loss: 2.9553 - mae: 1.1056 - val_loss: 8.8871 - val_mae: 2.1546\n",
            "Epoch 165/200\n",
            " - 0s - loss: 2.5038 - mae: 1.0050 - val_loss: 11.0798 - val_mae: 2.4448\n",
            "Epoch 166/200\n",
            " - 0s - loss: 2.9987 - mae: 1.1352 - val_loss: 9.6372 - val_mae: 2.2343\n",
            "Epoch 167/200\n",
            " - 0s - loss: 2.7039 - mae: 1.0952 - val_loss: 11.5269 - val_mae: 2.5171\n",
            "Epoch 168/200\n",
            " - 0s - loss: 2.6869 - mae: 1.0674 - val_loss: 10.3814 - val_mae: 2.2874\n",
            "Epoch 169/200\n",
            " - 1s - loss: 2.5511 - mae: 1.0698 - val_loss: 9.5942 - val_mae: 2.2451\n",
            "Epoch 170/200\n",
            " - 0s - loss: 2.6631 - mae: 1.0671 - val_loss: 10.8996 - val_mae: 2.4659\n",
            "Epoch 171/200\n",
            " - 0s - loss: 2.7393 - mae: 1.0498 - val_loss: 9.6629 - val_mae: 2.2583\n",
            "Epoch 172/200\n",
            " - 0s - loss: 2.7955 - mae: 1.0672 - val_loss: 11.7329 - val_mae: 2.5359\n",
            "Epoch 173/200\n",
            " - 0s - loss: 2.7219 - mae: 1.0845 - val_loss: 9.3292 - val_mae: 2.1944\n",
            "Epoch 174/200\n",
            " - 0s - loss: 2.5435 - mae: 1.0079 - val_loss: 10.5828 - val_mae: 2.3154\n",
            "Epoch 175/200\n",
            " - 0s - loss: 2.8630 - mae: 1.0825 - val_loss: 10.8329 - val_mae: 2.4172\n",
            "Epoch 176/200\n",
            " - 0s - loss: 2.6750 - mae: 1.0679 - val_loss: 9.7000 - val_mae: 2.2354\n",
            "Epoch 177/200\n",
            " - 1s - loss: 2.6284 - mae: 1.0665 - val_loss: 9.3488 - val_mae: 2.2544\n",
            "Epoch 178/200\n",
            " - 0s - loss: 2.6410 - mae: 1.0382 - val_loss: 9.4902 - val_mae: 2.2171\n",
            "Epoch 179/200\n",
            " - 0s - loss: 2.6302 - mae: 1.0746 - val_loss: 9.7499 - val_mae: 2.2789\n",
            "Epoch 180/200\n",
            " - 0s - loss: 2.4552 - mae: 1.0188 - val_loss: 9.5412 - val_mae: 2.2391\n",
            "Epoch 181/200\n",
            " - 0s - loss: 2.7723 - mae: 1.0123 - val_loss: 9.6586 - val_mae: 2.2197\n",
            "Epoch 182/200\n",
            " - 0s - loss: 2.5599 - mae: 1.0216 - val_loss: 9.9860 - val_mae: 2.2035\n",
            "Epoch 183/200\n",
            " - 0s - loss: 2.5604 - mae: 1.0834 - val_loss: 9.4603 - val_mae: 2.2836\n",
            "Epoch 184/200\n",
            " - 0s - loss: 2.4313 - mae: 1.0245 - val_loss: 9.6030 - val_mae: 2.2066\n",
            "Epoch 185/200\n",
            " - 0s - loss: 2.4258 - mae: 1.0458 - val_loss: 9.8328 - val_mae: 2.2812\n",
            "Epoch 186/200\n",
            " - 0s - loss: 2.5580 - mae: 1.0705 - val_loss: 9.8812 - val_mae: 2.2758\n",
            "Epoch 187/200\n",
            " - 0s - loss: 2.4826 - mae: 1.0674 - val_loss: 10.3642 - val_mae: 2.3880\n",
            "Epoch 188/200\n",
            " - 0s - loss: 2.4574 - mae: 1.0763 - val_loss: 10.6764 - val_mae: 2.3519\n",
            "Epoch 189/200\n",
            " - 1s - loss: 2.5071 - mae: 1.0214 - val_loss: 9.9320 - val_mae: 2.2236\n",
            "Epoch 190/200\n",
            " - 1s - loss: 2.4578 - mae: 1.0174 - val_loss: 10.2720 - val_mae: 2.3745\n",
            "Epoch 191/200\n",
            " - 0s - loss: 2.5006 - mae: 1.0158 - val_loss: 10.5500 - val_mae: 2.3831\n",
            "Epoch 192/200\n",
            " - 0s - loss: 2.2859 - mae: 1.0203 - val_loss: 10.2423 - val_mae: 2.2448\n",
            "Epoch 193/200\n",
            " - 0s - loss: 2.4125 - mae: 1.0423 - val_loss: 9.6880 - val_mae: 2.2627\n",
            "Epoch 194/200\n",
            " - 0s - loss: 2.3325 - mae: 0.9574 - val_loss: 13.3099 - val_mae: 2.7098\n",
            "Epoch 195/200\n",
            " - 0s - loss: 2.6494 - mae: 0.9813 - val_loss: 10.5565 - val_mae: 2.3488\n",
            "Epoch 196/200\n",
            " - 0s - loss: 2.4899 - mae: 1.0208 - val_loss: 9.4289 - val_mae: 2.2642\n",
            "Epoch 197/200\n",
            " - 0s - loss: 2.4907 - mae: 1.0234 - val_loss: 9.8330 - val_mae: 2.2250\n",
            "Epoch 198/200\n",
            " - 0s - loss: 2.1302 - mae: 0.9811 - val_loss: 10.9630 - val_mae: 2.3895\n",
            "Epoch 199/200\n",
            " - 0s - loss: 2.2530 - mae: 1.0250 - val_loss: 10.3888 - val_mae: 2.4029\n",
            "Epoch 200/200\n",
            " - 0s - loss: 2.2160 - mae: 0.9985 - val_loss: 9.8558 - val_mae: 2.1708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1EJEG6a9Ew-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ARqVvyMX9ExF"
      },
      "source": [
        "#### Takeaways\n",
        "* Regression is usually done using MSE loss and MAE for evaluation\n",
        "* Input data should always be scaled (independent from the test set)\n",
        "* Small datasets:\n",
        "    - Use cross-validation\n",
        "    - Use simple (non-deep) networks\n",
        "    - Smaller batches, more epochs"
      ]
    }
  ]
}